{
  "api/DotnetPrompt.Abstractions.Chains.ChainMessage.html": {
    "href": "api/DotnetPrompt.Abstractions.Chains.ChainMessage.html",
    "title": "Class ChainMessage | DotnetPrompt",
    "keywords": "Class ChainMessage Message that goes through chains Inheritance object ChainMessage Implements IEquatable<ChainMessage> Inherited Members object.Equals(object) object.Equals(object, object) object.GetHashCode() object.GetType() object.MemberwiseClone() object.ReferenceEquals(object, object) object.ToString() Namespace: DotnetPrompt.Abstractions.Chains Assembly: DotnetPrompt.Abstractions.dll Syntax public record ChainMessage : IEquatable<ChainMessage> Constructors | Improve this Doc View Source ChainMessage(IDictionary<string, string>, IList<string>) Message that goes through chains Declaration public ChainMessage(IDictionary<string, string> Values, IList<string> Stops = null) Parameters Type Name Description IDictionary<string, string> Values IList<string> Stops Properties | Improve this Doc View Source Id Declaration public Guid Id { get; set; } Property Value Type Description Guid | Improve this Doc View Source Stops Declaration public IList<string> Stops { get; init; } Property Value Type Description IList<string> | Improve this Doc View Source Values Declaration public IDictionary<string, string> Values { get; init; } Property Value Type Description IDictionary<string, string> Implements System.IEquatable<T>"
  },
  "api/DotnetPrompt.Abstractions.Chains.html": {
    "href": "api/DotnetPrompt.Abstractions.Chains.html",
    "title": "Namespace DotnetPrompt.Abstractions.Chains | DotnetPrompt",
    "keywords": "Namespace DotnetPrompt.Abstractions.Chains Classes ChainMessage Message that goes through chains Interfaces IChain Basic interface for a chain IChain<TInput, TOutput> Generic Chain IChainExecutor Chain executor to run chains"
  },
  "api/DotnetPrompt.Abstractions.Chains.IChain-2.html": {
    "href": "api/DotnetPrompt.Abstractions.Chains.IChain-2.html",
    "title": "Interface IChain<TInput, TOutput> | DotnetPrompt",
    "keywords": "Interface IChain<TInput, TOutput> Generic Chain Namespace: DotnetPrompt.Abstractions.Chains Assembly: DotnetPrompt.Abstractions.dll Syntax public interface IChain<TInput, TOutput> Type Parameters Name Description TInput TOutput Remarks Experimental, not yet used Properties | Improve this Doc View Source InputBlock Declaration ITargetBlock<TInput> InputBlock { get; } Property Value Type Description ITargetBlock<TInput> | Improve this Doc View Source OutputBlock Declaration ISourceBlock<TOutput> OutputBlock { get; } Property Value Type Description ISourceBlock<TOutput> Methods | Improve this Doc View Source Cancel() Cancel Chain execution Declaration void Cancel() | Improve this Doc View Source Run(TInput) Declaration bool Run(TInput message) Parameters Type Name Description TInput message Returns Type Description bool"
  },
  "api/DotnetPrompt.Abstractions.Chains.IChain.html": {
    "href": "api/DotnetPrompt.Abstractions.Chains.IChain.html",
    "title": "Interface IChain | DotnetPrompt",
    "keywords": "Interface IChain Basic interface for a chain Inherited Members IChain<ChainMessage, ChainMessage>.InputBlock IChain<ChainMessage, ChainMessage>.OutputBlock IChain<ChainMessage, ChainMessage>.Run(ChainMessage) IChain<ChainMessage, ChainMessage>.Cancel() Namespace: DotnetPrompt.Abstractions.Chains Assembly: DotnetPrompt.Abstractions.dll Syntax public interface IChain : IChain<ChainMessage, ChainMessage> Properties | Improve this Doc View Source DefaultOutputKey Output chain produces Declaration string DefaultOutputKey { get; set; } Property Value Type Description string | Improve this Doc View Source InputVariables List of inputs chain require to run Declaration IList<string> InputVariables { get; } Property Value Type Description IList<string>"
  },
  "api/DotnetPrompt.Abstractions.Chains.IChainExecutor.html": {
    "href": "api/DotnetPrompt.Abstractions.Chains.IChainExecutor.html",
    "title": "Interface IChainExecutor | DotnetPrompt",
    "keywords": "Interface IChainExecutor Chain executor to run chains Namespace: DotnetPrompt.Abstractions.Chains Assembly: DotnetPrompt.Abstractions.dll Syntax public interface IChainExecutor Methods | Improve this Doc View Source PromptAsync(IDictionary<string, string>, List<string>) Execute chain with default inputs and outputs Declaration Task<IDictionary<string, string>> PromptAsync(IDictionary<string, string> input, List<string> stops = null) Parameters Type Name Description IDictionary<string, string> input List<string> stops Returns Type Description Task<IDictionary<string, string>> | Improve this Doc View Source PromptAsync(string) Execute chain with single string input and single string output Declaration Task<string> PromptAsync(string input) Parameters Type Name Description string input Returns Type Description Task<string>"
  },
  "api/DotnetPrompt.Abstractions.LLM.BaseModel.html": {
    "href": "api/DotnetPrompt.Abstractions.LLM.BaseModel.html",
    "title": "Class BaseModel | DotnetPrompt",
    "keywords": "Class BaseModel Inheritance object BaseModel ChatGptModel OpenAIModel Implements ILargeLanguageModel Inherited Members object.Equals(object) object.Equals(object, object) object.GetHashCode() object.GetType() object.MemberwiseClone() object.ReferenceEquals(object, object) object.ToString() Namespace: DotnetPrompt.Abstractions.LLM Assembly: DotnetPrompt.Abstractions.dll Syntax public abstract class BaseModel : ILargeLanguageModel Constructors | Improve this Doc View Source BaseModel(ILogger, IDistributedCache) Declaration protected BaseModel(ILogger logger, IDistributedCache cache) Parameters Type Name Description ILogger logger IDistributedCache cache Fields | Improve this Doc View Source Logger Declaration protected readonly ILogger Logger Field Value Type Description ILogger Properties | Improve this Doc View Source DefaultStop Declaration public IList<string> DefaultStop { get; set; } Property Value Type Description IList<string> | Improve this Doc View Source LLMType Declaration public abstract string LLMType { get; } Property Value Type Description string | Improve this Doc View Source MaxRequestTokens Declaration public abstract int MaxRequestTokens { get; } Property Value Type Description int | Improve this Doc View Source UseCache Declaration public bool UseCache { get; set; } Property Value Type Description bool Methods | Improve this Doc View Source AsUniqueString() Return current model as unique string for caching purposes Declaration protected abstract string AsUniqueString() Returns Type Description string | Improve this Doc View Source GenerateAsync(IList<string>, IList<string>) Run the LLM on the given prompt and input. Declaration public Task<ModelResult> GenerateAsync(IList<string> prompts, IList<string> stop = null) Parameters Type Name Description IList<string> prompts IList<string> stop Returns Type Description Task<ModelResult> Exceptions Type Condition System.InvalidOperationException When cache asked without | Improve this Doc View Source GenerateInternalAsync(IList<string>, IList<string>) Declaration protected abstract Task<ModelResult> GenerateInternalAsync(IList<string> prompts, IList<string> stop = null) Parameters Type Name Description IList<string> prompts IList<string> stop Returns Type Description Task<ModelResult> | Improve this Doc View Source GetNumTokens(string) Get the number of tokens present in the text. Declaration public virtual int GetNumTokens(string text) Parameters Type Name Description string text Returns Type Description int Number of characters divided by 4 Implements ILargeLanguageModel Extension Methods ModelExtensions.PromptAsync(ILargeLanguageModel, string, List<string>)"
  },
  "api/DotnetPrompt.Abstractions.LLM.html": {
    "href": "api/DotnetPrompt.Abstractions.LLM.html",
    "title": "Namespace DotnetPrompt.Abstractions.LLM | DotnetPrompt",
    "keywords": "Namespace DotnetPrompt.Abstractions.LLM Classes BaseModel ModelCache Cache of requests to model based on prompt and model configuration ModelExtensions Interfaces ILargeLanguageModel"
  },
  "api/DotnetPrompt.Abstractions.LLM.ILargeLanguageModel.html": {
    "href": "api/DotnetPrompt.Abstractions.LLM.ILargeLanguageModel.html",
    "title": "Interface ILargeLanguageModel | DotnetPrompt",
    "keywords": "Interface ILargeLanguageModel Namespace: DotnetPrompt.Abstractions.LLM Assembly: DotnetPrompt.Abstractions.dll Syntax public interface ILargeLanguageModel Properties | Improve this Doc View Source LLMType Keyword for model type, used for serialization Declaration string LLMType { get; } Property Value Type Description string | Improve this Doc View Source MaxRequestTokens Maximum tokens that could be send to a language model Declaration int MaxRequestTokens { get; } Property Value Type Description int Methods | Improve this Doc View Source GenerateAsync(IList<string>, IList<string>) Run the LLM on the given prompts and stops. Declaration Task<ModelResult> GenerateAsync(IList<string> prompts, IList<string> stop = null) Parameters Type Name Description IList<string> prompts IList<string> stop Returns Type Description Task<ModelResult> Exceptions Type Condition System.InvalidOperationException When cache asked without Extension Methods ModelExtensions.PromptAsync(ILargeLanguageModel, string, List<string>)"
  },
  "api/DotnetPrompt.Abstractions.LLM.ModelCache.html": {
    "href": "api/DotnetPrompt.Abstractions.LLM.ModelCache.html",
    "title": "Class ModelCache | DotnetPrompt",
    "keywords": "Class ModelCache Cache of requests to model based on prompt and model configuration Inheritance object ModelCache Inherited Members object.Equals(object) object.Equals(object, object) object.GetHashCode() object.GetType() object.MemberwiseClone() object.ReferenceEquals(object, object) object.ToString() Namespace: DotnetPrompt.Abstractions.LLM Assembly: DotnetPrompt.Abstractions.dll Syntax public class ModelCache Constructors | Improve this Doc View Source ModelCache(IDistributedCache) Declaration public ModelCache(IDistributedCache cache) Parameters Type Name Description IDistributedCache cache Methods | Improve this Doc View Source GetPromptsAsync(string, IList<string>) Get prompts that are already cached. Declaration public Task<(Dictionary<int, IList<Generation>> ExistingPrompts, string LLMString, IList<int> MissingPromptIdxs, IList<string> MissingPrompts)> GetPromptsAsync(string llmString, IList<string> prompts) Parameters Type Name Description string llmString IList<string> prompts Returns Type Description Task<(Dictionary<int, IList<Generation>> ExistingPrompts, string LLMString, IList<int> MissingPromptIdxs, IList<string> MissingPrompts)> | Improve this Doc View Source UpdateCache(Dictionary<int, IList<Generation>>, string, IList<int>, ModelResult, IList<string>) Update the cache and get the LLM output. Declaration public Task<IDictionary<string, object>> UpdateCache(Dictionary<int, IList<Generation>> existingPrompts, string llmString, IList<int> missingPromptIndexes, ModelResult newResults, IList<string> prompts) Parameters Type Name Description Dictionary<int, IList<Generation>> existingPrompts string llmString IList<int> missingPromptIndexes ModelResult newResults IList<string> prompts Returns Type Description Task<IDictionary<string, object>>"
  },
  "api/DotnetPrompt.Abstractions.LLM.ModelExtensions.html": {
    "href": "api/DotnetPrompt.Abstractions.LLM.ModelExtensions.html",
    "title": "Class ModelExtensions | DotnetPrompt",
    "keywords": "Class ModelExtensions Inheritance object ModelExtensions Inherited Members object.Equals(object) object.Equals(object, object) object.GetHashCode() object.GetType() object.MemberwiseClone() object.ReferenceEquals(object, object) object.ToString() Namespace: DotnetPrompt.Abstractions.LLM Assembly: DotnetPrompt.Abstractions.dll Syntax public static class ModelExtensions Methods | Improve this Doc View Source PromptAsync(ILargeLanguageModel, string, List<string>) Run the LLM on the given prompt and input. Declaration public static Task<string> PromptAsync(this ILargeLanguageModel model, string prompt, List<string> stop = null) Parameters Type Name Description ILargeLanguageModel model string prompt List<string> stop Returns Type Description Task<string>"
  },
  "api/DotnetPrompt.Abstractions.LLM.Schema.Generation.html": {
    "href": "api/DotnetPrompt.Abstractions.LLM.Schema.Generation.html",
    "title": "Class Generation | DotnetPrompt",
    "keywords": "Class Generation Output of a single generation. Inheritance object Generation Implements IEquatable<Generation> Inherited Members object.Equals(object) object.Equals(object, object) object.GetHashCode() object.GetType() object.MemberwiseClone() object.ReferenceEquals(object, object) object.ToString() Namespace: DotnetPrompt.Abstractions.LLM.Schema Assembly: DotnetPrompt.Abstractions.dll Syntax public record Generation : IEquatable<Generation> Properties | Improve this Doc View Source Info Raw generation info response from the provider May include things like reason for finishing (e.g. in OpenAI) Declaration public Dictionary<string, object> Info { get; init; } Property Value Type Description Dictionary<string, object> | Improve this Doc View Source Text Generated text output. Declaration public string Text { get; init; } Property Value Type Description string Implements System.IEquatable<T>"
  },
  "api/DotnetPrompt.Abstractions.LLM.Schema.html": {
    "href": "api/DotnetPrompt.Abstractions.LLM.Schema.html",
    "title": "Namespace DotnetPrompt.Abstractions.LLM.Schema | DotnetPrompt",
    "keywords": "Namespace DotnetPrompt.Abstractions.LLM.Schema Classes Generation Output of a single generation. ModelResult Class that contains all relevant information for an LLM Result."
  },
  "api/DotnetPrompt.Abstractions.LLM.Schema.ModelResult.html": {
    "href": "api/DotnetPrompt.Abstractions.LLM.Schema.ModelResult.html",
    "title": "Class ModelResult | DotnetPrompt",
    "keywords": "Class ModelResult Class that contains all relevant information for an LLM Result. Inheritance object ModelResult Implements IEquatable<ModelResult> Inherited Members object.Equals(object) object.Equals(object, object) object.GetHashCode() object.GetType() object.MemberwiseClone() object.ReferenceEquals(object, object) object.ToString() Namespace: DotnetPrompt.Abstractions.LLM.Schema Assembly: DotnetPrompt.Abstractions.dll Syntax public record ModelResult : IEquatable<ModelResult> Properties | Improve this Doc View Source Generations List of the things generated. This is List[List[]] because each input could have multiple generations/generated completions. Declaration public IList<IList<Generation>> Generations { get; set; } Property Value Type Description IList<IList<Generation>> | Improve this Doc View Source Output For arbitrary LLM provider specific output. Declaration public IDictionary<string, object> Output { get; set; } Property Value Type Description IDictionary<string, object> Implements System.IEquatable<T>"
  },
  "api/DotnetPrompt.Abstractions.Prompts.html": {
    "href": "api/DotnetPrompt.Abstractions.Prompts.html",
    "title": "Namespace DotnetPrompt.Abstractions.Prompts | DotnetPrompt",
    "keywords": "Namespace DotnetPrompt.Abstractions.Prompts Interfaces IPromptTemplate Schema to represent a prompt for an LLM."
  },
  "api/DotnetPrompt.Abstractions.Prompts.IPromptTemplate.html": {
    "href": "api/DotnetPrompt.Abstractions.Prompts.IPromptTemplate.html",
    "title": "Interface IPromptTemplate | DotnetPrompt",
    "keywords": "Interface IPromptTemplate Schema to represent a prompt for an LLM. Namespace: DotnetPrompt.Abstractions.Prompts Assembly: DotnetPrompt.Abstractions.dll Syntax public interface IPromptTemplate Properties | Improve this Doc View Source InputVariables A list of the names of the variables the prompt template expects. Declaration IList<string> InputVariables { get; } Property Value Type Description IList<string> Methods | Improve this Doc View Source Format(IDictionary<string, string>) Build a prompt from current template and a list of values. Declaration string Format(IDictionary<string, string> values = null) Parameters Type Name Description IDictionary<string, string> values Key-Value list of values to use to build prompt. Returns Type Description string String prompt Exceptions Type Condition System.ArgumentException Throws when provided list of keys does not match InputVariables."
  },
  "api/DotnetPrompt.Abstractions.Tools.html": {
    "href": "api/DotnetPrompt.Abstractions.Tools.html",
    "title": "Namespace DotnetPrompt.Abstractions.Tools | DotnetPrompt",
    "keywords": "Namespace DotnetPrompt.Abstractions.Tools Classes TextEmbedding Interfaces IEmbeddings Interface for embedding models. ITokenizer Interface for tokenizer"
  },
  "api/DotnetPrompt.Abstractions.Tools.IEmbeddings.html": {
    "href": "api/DotnetPrompt.Abstractions.Tools.IEmbeddings.html",
    "title": "Interface IEmbeddings | DotnetPrompt",
    "keywords": "Interface IEmbeddings Interface for embedding models. Namespace: DotnetPrompt.Abstractions.Tools Assembly: DotnetPrompt.Abstractions.dll Syntax public interface IEmbeddings Methods | Improve this Doc View Source EmbedAsync(IList<string>) Embed search docs. Declaration Task<IList<TextEmbedding>> EmbedAsync(IList<string> texts) Parameters Type Name Description IList<string> texts Returns Type Description Task<IList<TextEmbedding>> | Improve this Doc View Source EmbedAsync(string) Embed query text. Declaration Task<TextEmbedding> EmbedAsync(string text) Parameters Type Name Description string text Returns Type Description Task<TextEmbedding>"
  },
  "api/DotnetPrompt.Abstractions.Tools.ITokenizer.html": {
    "href": "api/DotnetPrompt.Abstractions.Tools.ITokenizer.html",
    "title": "Interface ITokenizer | DotnetPrompt",
    "keywords": "Interface ITokenizer Interface for tokenizer Namespace: DotnetPrompt.Abstractions.Tools Assembly: DotnetPrompt.Abstractions.dll Syntax public interface ITokenizer Remarks A tokenizer is a software component that breaks down a piece of text into smaller units called tokens. Methods | Improve this Doc View Source Encode(string) Encode text as a list of numerical tokens Declaration IList<int> Encode(string text) Parameters Type Name Description string text Returns Type Description IList<int>"
  },
  "api/DotnetPrompt.Abstractions.Tools.TextEmbedding.html": {
    "href": "api/DotnetPrompt.Abstractions.Tools.TextEmbedding.html",
    "title": "Class TextEmbedding | DotnetPrompt",
    "keywords": "Class TextEmbedding Inheritance object TextEmbedding Implements IEquatable<TextEmbedding> Inherited Members object.Equals(object) object.Equals(object, object) object.GetHashCode() object.GetType() object.MemberwiseClone() object.ReferenceEquals(object, object) object.ToString() Namespace: DotnetPrompt.Abstractions.Tools Assembly: DotnetPrompt.Abstractions.dll Syntax public record TextEmbedding : IEquatable<TextEmbedding> Constructors | Improve this Doc View Source TextEmbedding(IReadOnlyList<float>, string) Declaration public TextEmbedding(IReadOnlyList<float> Embedding, string Text) Parameters Type Name Description IReadOnlyList<float> Embedding string Text Properties | Improve this Doc View Source Embedding Declaration public IReadOnlyList<float> Embedding { get; init; } Property Value Type Description IReadOnlyList<float> | Improve this Doc View Source Text Declaration public string Text { get; init; } Property Value Type Description string Implements System.IEquatable<T>"
  },
  "api/DotnetPrompt.Chains.ChainExecutorExtensions.html": {
    "href": "api/DotnetPrompt.Chains.ChainExecutorExtensions.html",
    "title": "Class ChainExecutorExtensions | DotnetPrompt",
    "keywords": "Class ChainExecutorExtensions Extension class for IChain Inheritance object ChainExecutorExtensions Inherited Members object.Equals(object) object.Equals(object, object) object.GetHashCode() object.GetType() object.MemberwiseClone() object.ReferenceEquals(object, object) object.ToString() Namespace: DotnetPrompt.Chains Assembly: DotnetPrompt.dll Syntax public static class ChainExecutorExtensions Methods | Improve this Doc View Source GetExecutor(IChain, bool) Create chain executor for IChain Declaration public static IChainExecutor GetExecutor(this IChain chain, bool oneShot = true) Parameters Type Name Description IChain chain bool oneShot Returns Type Description IChainExecutor IChainExecutor instance to execute chain Exceptions Type Condition System.NotImplementedException"
  },
  "api/DotnetPrompt.Chains.ChainExtensions.html": {
    "href": "api/DotnetPrompt.Chains.ChainExtensions.html",
    "title": "Class ChainExtensions | DotnetPrompt",
    "keywords": "Class ChainExtensions Extension class for IChain Inheritance object ChainExtensions Inherited Members object.Equals(object) object.Equals(object, object) object.GetHashCode() object.GetType() object.MemberwiseClone() object.ReferenceEquals(object, object) object.ToString() Namespace: DotnetPrompt.Chains Assembly: DotnetPrompt.dll Syntax public static class ChainExtensions Methods | Improve this Doc View Source LinkTo(IChain, IChain) Link several chain together Declaration public static IDisposable LinkTo(this IChain source, IChain target) Parameters Type Name Description IChain source IChain target Returns Type Description IDisposable Exceptions Type Condition System.InvalidOperationException Source output does not match target input"
  },
  "api/DotnetPrompt.Chains.html": {
    "href": "api/DotnetPrompt.Chains.html",
    "title": "Namespace DotnetPrompt.Chains | DotnetPrompt",
    "keywords": "Namespace DotnetPrompt.Chains Classes ChainExecutorExtensions Extension class for IChain ChainExtensions Extension class for IChain ModelChain LLM chain OneShotChainExecutor Chain executor for a single run"
  },
  "api/DotnetPrompt.Chains.ModelChain.html": {
    "href": "api/DotnetPrompt.Chains.ModelChain.html",
    "title": "Class ModelChain | DotnetPrompt",
    "keywords": "Class ModelChain LLM chain Inheritance object ModelChain ConversationChain QuestionAnsweringChain SummarizeChain Implements IChain IChain<ChainMessage, ChainMessage> Inherited Members object.Equals(object) object.Equals(object, object) object.GetHashCode() object.GetType() object.MemberwiseClone() object.ReferenceEquals(object, object) object.ToString() Namespace: DotnetPrompt.Chains Assembly: DotnetPrompt.dll Syntax public class ModelChain : IChain, IChain<ChainMessage, ChainMessage> Constructors | Improve this Doc View Source ModelChain(IPromptTemplate, ILargeLanguageModel, string, ILogger?) Declaration public ModelChain(IPromptTemplate prompt, ILargeLanguageModel llm, string defaultOutputKey = \"text\", ILogger? logger = null) Parameters Type Name Description IPromptTemplate prompt Prompt template for chain to use in LLM ILargeLanguageModel llm LLM to use string defaultOutputKey Default key for chain result ILogger logger Default logger Properties | Improve this Doc View Source DefaultOutputKey Output chain produces Declaration public string DefaultOutputKey { get; set; } Property Value Type Description string | Improve this Doc View Source InputBlock Declaration public ITargetBlock<ChainMessage> InputBlock { get; } Property Value Type Description ITargetBlock<ChainMessage> | Improve this Doc View Source InputVariables List of inputs chain require to run Declaration public IList<string> InputVariables { get; } Property Value Type Description IList<string> | Improve this Doc View Source ModelMaxRequestTokens Maximum number of tokens Declaration public int ModelMaxRequestTokens { get; } Property Value Type Description int | Improve this Doc View Source OutputBlock Declaration public ISourceBlock<ChainMessage> OutputBlock { get; } Property Value Type Description ISourceBlock<ChainMessage> Methods | Improve this Doc View Source Cancel() Cancel Chain execution Declaration public void Cancel() Implements IChain IChain<TInput, TOutput> Extension Methods ChainExecutorExtensions.GetExecutor(IChain, bool) ChainExtensions.LinkTo(IChain, IChain)"
  },
  "api/DotnetPrompt.Chains.OneShotChainExecutor.html": {
    "href": "api/DotnetPrompt.Chains.OneShotChainExecutor.html",
    "title": "Class OneShotChainExecutor | DotnetPrompt",
    "keywords": "Class OneShotChainExecutor Chain executor for a single run Inheritance object OneShotChainExecutor Implements IChainExecutor Inherited Members object.Equals(object) object.Equals(object, object) object.GetHashCode() object.GetType() object.MemberwiseClone() object.ReferenceEquals(object, object) object.ToString() Namespace: DotnetPrompt.Chains Assembly: DotnetPrompt.dll Syntax public class OneShotChainExecutor : IChainExecutor Constructors | Improve this Doc View Source OneShotChainExecutor(IChain) ctor Declaration public OneShotChainExecutor(IChain chainToExecute) Parameters Type Name Description IChain chainToExecute Methods | Improve this Doc View Source PromptAsync(IDictionary<string, string>, List<string>?) Execute chain with default inputs and outputs Declaration public Task<IDictionary<string, string>> PromptAsync(IDictionary<string, string> input, List<string>? stops = null) Parameters Type Name Description IDictionary<string, string> input List<string> stops Returns Type Description Task<IDictionary<string, string>> | Improve this Doc View Source PromptAsync(string) Execute chain with single string input and single string output Declaration public Task<string> PromptAsync(string input) Parameters Type Name Description string input Returns Type Description Task<string> Implements IChainExecutor"
  },
  "api/DotnetPrompt.Chains.Specialized.ConversationChain.html": {
    "href": "api/DotnetPrompt.Chains.Specialized.ConversationChain.html",
    "title": "Class ConversationChain | DotnetPrompt",
    "keywords": "Class ConversationChain Simple pre-built chain for conversation generation Inheritance object ModelChain ConversationChain Implements IChain IChain<ChainMessage, ChainMessage> Inherited Members ModelChain.InputVariables ModelChain.DefaultOutputKey ModelChain.InputBlock ModelChain.OutputBlock ModelChain.Cancel() ModelChain.ModelMaxRequestTokens object.Equals(object) object.Equals(object, object) object.GetHashCode() object.GetType() object.MemberwiseClone() object.ReferenceEquals(object, object) object.ToString() Namespace: DotnetPrompt.Chains.Specialized Assembly: DotnetPrompt.dll Syntax public class ConversationChain : ModelChain, IChain, IChain<ChainMessage, ChainMessage> Constructors | Improve this Doc View Source ConversationChain(ILargeLanguageModel, ILogger<ConversationChain>) ctor Declaration public ConversationChain(ILargeLanguageModel llm, ILogger<ConversationChain> logger = null) Parameters Type Name Description ILargeLanguageModel llm ILogger<ConversationChain> logger Implements IChain IChain<TInput, TOutput> Extension Methods ChainExecutorExtensions.GetExecutor(IChain, bool) ChainExtensions.LinkTo(IChain, IChain)"
  },
  "api/DotnetPrompt.Chains.Specialized.html": {
    "href": "api/DotnetPrompt.Chains.Specialized.html",
    "title": "Namespace DotnetPrompt.Chains.Specialized | DotnetPrompt",
    "keywords": "Namespace DotnetPrompt.Chains.Specialized Classes ConversationChain Simple pre-built chain for conversation generation MapReduceChain QuestionAnsweringChain Chain to answer question based on input context SequentialChain Chain that represent sequential list of chains SummarizeChain A simple summarize chain"
  },
  "api/DotnetPrompt.Chains.Specialized.MapReduceChain.html": {
    "href": "api/DotnetPrompt.Chains.Specialized.MapReduceChain.html",
    "title": "Class MapReduceChain | DotnetPrompt",
    "keywords": "Class MapReduceChain Inheritance object MapReduceChain Implements IChain IChain<ChainMessage, ChainMessage> Inherited Members object.Equals(object) object.Equals(object, object) object.GetHashCode() object.GetType() object.MemberwiseClone() object.ReferenceEquals(object, object) object.ToString() Namespace: DotnetPrompt.Chains.Specialized Assembly: DotnetPrompt.dll Syntax public class MapReduceChain : IChain, IChain<ChainMessage, ChainMessage> Constructors | Improve this Doc View Source MapReduceChain(IChain, IChain, Func<string, IEnumerable<string>>?, Func<IEnumerable<string>, string>?, Func<IEnumerable<string>, IEnumerable<string>>?, Func<string, bool>?) Declaration public MapReduceChain(IChain mapChain, IChain reduceChain, Func<string, IEnumerable<string>>? chunkFunc = null, Func<IEnumerable<string>, string>? mergeFunc = null, Func<IEnumerable<string>, IEnumerable<string>>? sortFunc = null, Func<string, bool>? fitReduceChain = null) Parameters Type Name Description IChain mapChain IChain reduceChain Func<string, IEnumerable<string>> chunkFunc Func<IEnumerable<string>, string> mergeFunc Func<IEnumerable<string>, IEnumerable<string>> sortFunc Func<string, bool> fitReduceChain Properties | Improve this Doc View Source DefaultOutputKey Output chain produces Declaration public string DefaultOutputKey { get; set; } Property Value Type Description string | Improve this Doc View Source InputBlock Declaration public ITargetBlock<ChainMessage> InputBlock { get; } Property Value Type Description ITargetBlock<ChainMessage> | Improve this Doc View Source InputVariables List of inputs chain require to run Declaration public IList<string> InputVariables { get; } Property Value Type Description IList<string> | Improve this Doc View Source MaxTokens Declaration public int MaxTokens { get; set; } Property Value Type Description int | Improve this Doc View Source OutputBlock Declaration public ISourceBlock<ChainMessage> OutputBlock { get; } Property Value Type Description ISourceBlock<ChainMessage> Methods | Improve this Doc View Source Cancel() Cancel Chain execution Declaration public void Cancel() Implements IChain IChain<TInput, TOutput> Extension Methods ChainExecutorExtensions.GetExecutor(IChain, bool) ChainExtensions.LinkTo(IChain, IChain)"
  },
  "api/DotnetPrompt.Chains.Specialized.QuestionAnsweringChain.html": {
    "href": "api/DotnetPrompt.Chains.Specialized.QuestionAnsweringChain.html",
    "title": "Class QuestionAnsweringChain | DotnetPrompt",
    "keywords": "Class QuestionAnsweringChain Chain to answer question based on input context Inheritance object ModelChain QuestionAnsweringChain Implements IChain IChain<ChainMessage, ChainMessage> Inherited Members ModelChain.InputVariables ModelChain.DefaultOutputKey ModelChain.InputBlock ModelChain.OutputBlock ModelChain.Cancel() ModelChain.ModelMaxRequestTokens object.Equals(object) object.Equals(object, object) object.GetHashCode() object.GetType() object.MemberwiseClone() object.ReferenceEquals(object, object) object.ToString() Namespace: DotnetPrompt.Chains.Specialized Assembly: DotnetPrompt.dll Syntax public class QuestionAnsweringChain : ModelChain, IChain, IChain<ChainMessage, ChainMessage> Remarks Input Variables: context, question Output Variables: answer Constructors | Improve this Doc View Source QuestionAnsweringChain(ILargeLanguageModel, ILogger<ModelChain>?) ctor Declaration public QuestionAnsweringChain(ILargeLanguageModel llm, ILogger<ModelChain>? logger = null) Parameters Type Name Description ILargeLanguageModel llm ILogger<ModelChain> logger Implements IChain IChain<TInput, TOutput> Extension Methods ChainExecutorExtensions.GetExecutor(IChain, bool) ChainExtensions.LinkTo(IChain, IChain)"
  },
  "api/DotnetPrompt.Chains.Specialized.SequentialChain.html": {
    "href": "api/DotnetPrompt.Chains.Specialized.SequentialChain.html",
    "title": "Class SequentialChain | DotnetPrompt",
    "keywords": "Class SequentialChain Chain that represent sequential list of chains Inheritance object SequentialChain Implements IChain IChain<ChainMessage, ChainMessage> Inherited Members object.Equals(object) object.Equals(object, object) object.GetHashCode() object.GetType() object.MemberwiseClone() object.ReferenceEquals(object, object) object.ToString() Namespace: DotnetPrompt.Chains.Specialized Assembly: DotnetPrompt.dll Syntax public class SequentialChain : IChain, IChain<ChainMessage, ChainMessage> Constructors | Improve this Doc View Source SequentialChain(IReadOnlyList<IChain>) ctor Declaration public SequentialChain(IReadOnlyList<IChain> chains) Parameters Type Name Description IReadOnlyList<IChain> chains List of chain that would be linked together where input should match output Properties | Improve this Doc View Source Chains List of inner chains of SequentialChain Declaration public IReadOnlyList<IChain> Chains { get; } Property Value Type Description IReadOnlyList<IChain> | Improve this Doc View Source DefaultOutputKey Output chain produces Declaration public string DefaultOutputKey { get; set; } Property Value Type Description string | Improve this Doc View Source InputBlock Declaration public ITargetBlock<ChainMessage> InputBlock { get; } Property Value Type Description ITargetBlock<ChainMessage> | Improve this Doc View Source InputVariables List of inputs chain require to run Declaration public IList<string> InputVariables { get; } Property Value Type Description IList<string> | Improve this Doc View Source OutputBlock Declaration public ISourceBlock<ChainMessage> OutputBlock { get; } Property Value Type Description ISourceBlock<ChainMessage> Methods | Improve this Doc View Source Cancel() Cancel Chain execution Declaration public void Cancel() Implements IChain IChain<TInput, TOutput> Extension Methods ChainExecutorExtensions.GetExecutor(IChain, bool) ChainExtensions.LinkTo(IChain, IChain)"
  },
  "api/DotnetPrompt.Chains.Specialized.SummarizeChain.html": {
    "href": "api/DotnetPrompt.Chains.Specialized.SummarizeChain.html",
    "title": "Class SummarizeChain | DotnetPrompt",
    "keywords": "Class SummarizeChain A simple summarize chain Inheritance object ModelChain SummarizeChain Implements IChain IChain<ChainMessage, ChainMessage> Inherited Members ModelChain.InputVariables ModelChain.DefaultOutputKey ModelChain.InputBlock ModelChain.OutputBlock ModelChain.Cancel() ModelChain.ModelMaxRequestTokens object.Equals(object) object.Equals(object, object) object.GetHashCode() object.GetType() object.MemberwiseClone() object.ReferenceEquals(object, object) object.ToString() Namespace: DotnetPrompt.Chains.Specialized Assembly: DotnetPrompt.dll Syntax public class SummarizeChain : ModelChain, IChain, IChain<ChainMessage, ChainMessage> Constructors | Improve this Doc View Source SummarizeChain(ILargeLanguageModel, ILogger?) ctor Declaration public SummarizeChain(ILargeLanguageModel llm, ILogger? logger = null) Parameters Type Name Description ILargeLanguageModel llm ILogger logger Implements IChain IChain<TInput, TOutput> Extension Methods ChainExecutorExtensions.GetExecutor(IChain, bool) ChainExtensions.LinkTo(IChain, IChain)"
  },
  "api/DotnetPrompt.LLM.OpenAI.AzureOpenAIModel.html": {
    "href": "api/DotnetPrompt.LLM.OpenAI.AzureOpenAIModel.html",
    "title": "Class AzureOpenAIModel | DotnetPrompt",
    "keywords": "Class AzureOpenAIModel OpenAI implementation to communicate with Azure version of OpenAI Inheritance object BaseModel OpenAIModel AzureOpenAIModel Implements ILargeLanguageModel Inherited Members OpenAIModel.DefaultModelConfiguration OpenAIModel.OpenAiApiKey OpenAIModel.BatchSize OpenAIModel.Streaming OpenAIModel.ModelExtraArguments OpenAIModel.CompletionWithRetry(OpenAIModelConfiguration) OpenAIModel.MaxTokensForPrompt(string) OpenAIModel.GetNumTokens(string) OpenAIModel.LLMType OpenAIModel.MaxRequestTokens OpenAIModel.ModelNameToContextSize(string) OpenAIModel.GenerateInternalAsync(IList<string>, IList<string>) OpenAIModel.GetSubPrompts(OpenAIModelConfiguration, IList<string>) BaseModel.Logger BaseModel.DefaultStop BaseModel.UseCache BaseModel.GenerateAsync(IList<string>, IList<string>) object.Equals(object) object.Equals(object, object) object.GetHashCode() object.GetType() object.MemberwiseClone() object.ReferenceEquals(object, object) object.ToString() Namespace: DotnetPrompt.LLM.OpenAI Assembly: DotnetPrompt.LLM.OpenAI.dll Syntax public class AzureOpenAIModel : OpenAIModel, ILargeLanguageModel Constructors | Improve this Doc View Source AzureOpenAIModel(IConfiguration, ILogger<AzureOpenAIModel>, IDistributedCache) Declaration public AzureOpenAIModel(IConfiguration configuration, ILogger<AzureOpenAIModel> logger, IDistributedCache cache) Parameters Type Name Description IConfiguration configuration ILogger<AzureOpenAIModel> logger IDistributedCache cache | Improve this Doc View Source AzureOpenAIModel(string, string, string, OpenAIModelConfiguration, ILogger) Declaration public AzureOpenAIModel(string openAIApiKey, string deploymentId, string account, OpenAIModelConfiguration defaultModelConfiguration, ILogger logger = null) Parameters Type Name Description string openAIApiKey string deploymentId string account OpenAIModelConfiguration defaultModelConfiguration ILogger logger Properties | Improve this Doc View Source Account Declaration public string Account { get; } Property Value Type Description string | Improve this Doc View Source DeploymentId Declaration public string DeploymentId { get; } Property Value Type Description string Methods | Improve this Doc View Source AsUniqueString() Declaration protected override string AsUniqueString() Returns Type Description string Overrides OpenAIModel.AsUniqueString() | Improve this Doc View Source CompletionsResponseValue(OpenAIModelConfiguration) Declaration protected override Task<Completions> CompletionsResponseValue(OpenAIModelConfiguration options) Parameters Type Name Description OpenAIModelConfiguration options Returns Type Description Task<Completions> Overrides OpenAIModel.CompletionsResponseValue(OpenAIModelConfiguration) Implements ILargeLanguageModel"
  },
  "api/DotnetPrompt.LLM.OpenAI.ChatGptModel.html": {
    "href": "api/DotnetPrompt.LLM.OpenAI.ChatGptModel.html",
    "title": "Class ChatGptModel | DotnetPrompt",
    "keywords": "Class ChatGptModel OpenAI model to use with gpt-3.5-turbo model Inheritance object BaseModel ChatGptModel Implements ILargeLanguageModel Inherited Members BaseModel.Logger BaseModel.DefaultStop BaseModel.UseCache BaseModel.GenerateAsync(IList<string>, IList<string>) BaseModel.GetNumTokens(string) object.Equals(object) object.Equals(object, object) object.GetHashCode() object.GetType() object.MemberwiseClone() object.ReferenceEquals(object, object) object.ToString() Namespace: DotnetPrompt.LLM.OpenAI Assembly: DotnetPrompt.LLM.OpenAI.dll Syntax public class ChatGptModel : BaseModel, ILargeLanguageModel Constructors | Improve this Doc View Source ChatGptModel(string, ChatGptModelConfiguration, ILogger, IDistributedCache) Default constructor without logging Declaration public ChatGptModel(string openAIApiKey, ChatGptModelConfiguration defaultModelConfiguration, ILogger logger = null, IDistributedCache cache = null) Parameters Type Name Description string openAIApiKey ChatGptModelConfiguration defaultModelConfiguration ILogger logger IDistributedCache cache Fields | Improve this Doc View Source Model Declaration public const string Model = \"gpt-3.5-turbo\" Field Value Type Description string Properties | Improve this Doc View Source DefaultModelConfiguration Declaration public ChatGptModelConfiguration DefaultModelConfiguration { get; set; } Property Value Type Description ChatGptModelConfiguration | Improve this Doc View Source LLMType Declaration public override string LLMType { get; } Property Value Type Description string Overrides BaseModel.LLMType | Improve this Doc View Source MaxRequestTokens Declaration public override int MaxRequestTokens { get; } Property Value Type Description int Overrides BaseModel.MaxRequestTokens | Improve this Doc View Source OpenAiApiKey Declaration public string OpenAiApiKey { get; set; } Property Value Type Description string Methods | Improve this Doc View Source AsUniqueString() Declaration protected override string AsUniqueString() Returns Type Description string Overrides BaseModel.AsUniqueString() | Improve this Doc View Source CompletionsResponseValue(ChatGptModelConfiguration) Declaration protected Task<Completions> CompletionsResponseValue(ChatGptModelConfiguration options) Parameters Type Name Description ChatGptModelConfiguration options Returns Type Description Task<Completions> | Improve this Doc View Source GenerateInternalAsync(IList<string>, IList<string>) Declaration protected override Task<ModelResult> GenerateInternalAsync(IList<string> prompts, IList<string> stop = null) Parameters Type Name Description IList<string> prompts IList<string> stop Returns Type Description Task<ModelResult> Overrides BaseModel.GenerateInternalAsync(IList<string>, IList<string>) | Improve this Doc View Source ValidateJSON(string) Declaration public static bool ValidateJSON(string jsonString) Parameters Type Name Description string jsonString Returns Type Description bool Implements ILargeLanguageModel"
  },
  "api/DotnetPrompt.LLM.OpenAI.ChatGptModelConfiguration.html": {
    "href": "api/DotnetPrompt.LLM.OpenAI.ChatGptModelConfiguration.html",
    "title": "Class ChatGptModelConfiguration | DotnetPrompt",
    "keywords": "Class ChatGptModelConfiguration Inheritance object OpenAIModelConfiguration ChatGptModelConfiguration Implements IEquatable<OpenAIModelConfiguration> IEquatable<ChatGptModelConfiguration> Inherited Members OpenAIModelConfiguration.NucleusSamplingFactor OpenAIModelConfiguration.SnippetCount OpenAIModelConfiguration.LogProbability OpenAIModelConfiguration.GenerationSampleCount OpenAIModelConfiguration.Prompt OpenAIModelConfiguration.MaxTokens OpenAIModelConfiguration.Temperature OpenAIModelConfiguration.LogitBias OpenAIModelConfiguration.User OpenAIModelConfiguration.Model OpenAIModelConfiguration.Echo OpenAIModelConfiguration.Stop OpenAIModelConfiguration.CompletionConfig OpenAIModelConfiguration.CacheLevel OpenAIModelConfiguration.PresencePenalty OpenAIModelConfiguration.FrequencyPenalty object.Equals(object) object.Equals(object, object) object.GetHashCode() object.GetType() object.MemberwiseClone() object.ReferenceEquals(object, object) object.ToString() Namespace: DotnetPrompt.LLM.OpenAI Assembly: DotnetPrompt.LLM.OpenAI.dll Syntax public record ChatGptModelConfiguration : OpenAIModelConfiguration, IEquatable<OpenAIModelConfiguration>, IEquatable<ChatGptModelConfiguration> Fields | Improve this Doc View Source Default Declaration public static ChatGptModelConfiguration Default Field Value Type Description ChatGptModelConfiguration Properties | Improve this Doc View Source Messages Declaration [JsonPropertyName(\"messages\")] public IList<ChatMessage> Messages { get; set; } Property Value Type Description IList<ChatMessage> Implements System.IEquatable<T> System.IEquatable<T>"
  },
  "api/DotnetPrompt.LLM.OpenAI.DependencyInjection.html": {
    "href": "api/DotnetPrompt.LLM.OpenAI.DependencyInjection.html",
    "title": "Namespace DotnetPrompt.LLM.OpenAI.DependencyInjection | DotnetPrompt",
    "keywords": "Namespace DotnetPrompt.LLM.OpenAI.DependencyInjection Classes OpenAIServiceCollectionExtensions Provides extension methods for registering OpenAIModel and AzureOpenAIModel in an Microsoft.Extensions.DependencyInjection.IServiceCollection."
  },
  "api/DotnetPrompt.LLM.OpenAI.DependencyInjection.OpenAIServiceCollectionExtensions.html": {
    "href": "api/DotnetPrompt.LLM.OpenAI.DependencyInjection.OpenAIServiceCollectionExtensions.html",
    "title": "Class OpenAIServiceCollectionExtensions | DotnetPrompt",
    "keywords": "Class OpenAIServiceCollectionExtensions Provides extension methods for registering OpenAIModel and AzureOpenAIModel in an Microsoft.Extensions.DependencyInjection.IServiceCollection. Inheritance object OpenAIServiceCollectionExtensions Inherited Members object.Equals(object) object.Equals(object, object) object.GetHashCode() object.GetType() object.MemberwiseClone() object.ReferenceEquals(object, object) object.ToString() Namespace: DotnetPrompt.LLM.OpenAI.DependencyInjection Assembly: DotnetPrompt.LLM.OpenAI.dll Syntax public static class OpenAIServiceCollectionExtensions Methods | Improve this Doc View Source AddAzureOpenAIModel(IServiceCollection) Register the dependencies AzureOpenAIModel and AzureOpenAIEmbeddings Declaration public static IServiceCollection AddAzureOpenAIModel(this IServiceCollection services) Parameters Type Name Description IServiceCollection services Returns Type Description IServiceCollection | Improve this Doc View Source AddOpenAIModel(IServiceCollection) Register the dependencies OpenAIModel and OpenAIEmbeddings Declaration public static IServiceCollection AddOpenAIModel(this IServiceCollection services) Parameters Type Name Description IServiceCollection services Returns Type Description IServiceCollection"
  },
  "api/DotnetPrompt.LLM.OpenAI.html": {
    "href": "api/DotnetPrompt.LLM.OpenAI.html",
    "title": "Namespace DotnetPrompt.LLM.OpenAI | DotnetPrompt",
    "keywords": "Namespace DotnetPrompt.LLM.OpenAI Classes AzureOpenAIModel OpenAI implementation to communicate with Azure version of OpenAI ChatGptModel OpenAI model to use with gpt-3.5-turbo model ChatGptModelConfiguration OpenAIEmbeddings Embeddings client for OpenAI OpenAIModel Model for OpenAI OpenAIModelConfiguration Tokens"
  },
  "api/DotnetPrompt.LLM.OpenAI.Model.ChatMessage.html": {
    "href": "api/DotnetPrompt.LLM.OpenAI.Model.ChatMessage.html",
    "title": "Class ChatMessage | DotnetPrompt",
    "keywords": "Class ChatMessage Inheritance object ChatMessage Implements IEquatable<ChatMessage> Inherited Members object.Equals(object) object.Equals(object, object) object.GetHashCode() object.GetType() object.MemberwiseClone() object.ReferenceEquals(object, object) object.ToString() Namespace: DotnetPrompt.LLM.OpenAI.Model Assembly: DotnetPrompt.LLM.OpenAI.dll Syntax public record ChatMessage : IEquatable<ChatMessage> Constructors | Improve this Doc View Source ChatMessage(string, string) Declaration public ChatMessage(string role, string content) Parameters Type Name Description string role string content Properties | Improve this Doc View Source Content Declaration [JsonPropertyName(\"content\")] public string Content { get; init; } Property Value Type Description string | Improve this Doc View Source Role Declaration [JsonPropertyName(\"role\")] public string Role { get; init; } Property Value Type Description string Implements System.IEquatable<T>"
  },
  "api/DotnetPrompt.LLM.OpenAI.Model.Choice.html": {
    "href": "api/DotnetPrompt.LLM.OpenAI.Model.Choice.html",
    "title": "Class Choice | DotnetPrompt",
    "keywords": "Class Choice Inheritance object Choice Implements IEquatable<Choice> Inherited Members object.Equals(object) object.Equals(object, object) object.GetHashCode() object.GetType() object.MemberwiseClone() object.ReferenceEquals(object, object) object.ToString() Namespace: DotnetPrompt.LLM.OpenAI.Model Assembly: DotnetPrompt.LLM.OpenAI.dll Syntax public record Choice : IEquatable<Choice> Properties | Improve this Doc View Source FinishReason Reason for finishing. Declaration [JsonPropertyName(\"finish_reason\")] public string FinishReason { get; init; } Property Value Type Description string | Improve this Doc View Source Index Index. Declaration [JsonPropertyName(\"index\")] public int? Index { get; init; } Property Value Type Description int? | Improve this Doc View Source Logprobs Log Prob Model. Declaration [JsonPropertyName(\"logprobs\")] public CompletionsLogProbability Logprobs { get; init; } Property Value Type Description CompletionsLogProbability | Improve this Doc View Source Message The generated completions in the chat format. Declaration [JsonPropertyName(\"message\")] public ChatMessage Message { get; init; } Property Value Type Description ChatMessage | Improve this Doc View Source Text Generated text for given completion prompt. Declaration [JsonPropertyName(\"text\")] public string Text { get; init; } Property Value Type Description string Implements System.IEquatable<T>"
  },
  "api/DotnetPrompt.LLM.OpenAI.Model.Completions.html": {
    "href": "api/DotnetPrompt.LLM.OpenAI.Model.Completions.html",
    "title": "Class Completions | DotnetPrompt",
    "keywords": "Class Completions Expected response schema to completion request. Inheritance object Completions Implements IEquatable<Completions> Inherited Members object.Equals(object) object.Equals(object, object) object.GetHashCode() object.GetType() object.MemberwiseClone() object.ReferenceEquals(object, object) object.ToString() Namespace: DotnetPrompt.LLM.OpenAI.Model Assembly: DotnetPrompt.LLM.OpenAI.dll Syntax public record Completions : IEquatable<Completions> Properties | Improve this Doc View Source Choices Array of choices returned containing text completions to prompts sent. Declaration [JsonPropertyName(\"choices\")] public IReadOnlyList<Choice> Choices { get; init; } Property Value Type Description IReadOnlyList<Choice> | Improve this Doc View Source Created Created time for completion response. Declaration [JsonPropertyName(\"created\")] public int? Created { get; init; } Property Value Type Description int? | Improve this Doc View Source Id Id for completion response. Declaration [JsonPropertyName(\"id\")] public string Id { get; init; } Property Value Type Description string | Improve this Doc View Source Model Model used for completion response. Declaration [JsonPropertyName(\"model\")] public string Model { get; init; } Property Value Type Description string | Improve this Doc View Source Usage Usage counts for tokens input using the completions API. Declaration [JsonPropertyName(\"usage\")] public CompletionsUsage Usage { get; init; } Property Value Type Description CompletionsUsage Implements System.IEquatable<T>"
  },
  "api/DotnetPrompt.LLM.OpenAI.Model.CompletionsLogProbability.html": {
    "href": "api/DotnetPrompt.LLM.OpenAI.Model.CompletionsLogProbability.html",
    "title": "Class CompletionsLogProbability | DotnetPrompt",
    "keywords": "Class CompletionsLogProbability Inheritance object CompletionsLogProbability Implements IEquatable<CompletionsLogProbability> Inherited Members object.Equals(object) object.Equals(object, object) object.GetHashCode() object.GetType() object.MemberwiseClone() object.ReferenceEquals(object, object) object.ToString() Namespace: DotnetPrompt.LLM.OpenAI.Model Assembly: DotnetPrompt.LLM.OpenAI.dll Syntax public record CompletionsLogProbability : IEquatable<CompletionsLogProbability> Properties | Improve this Doc View Source TextOffset Text offset. Declaration [JsonPropertyName(\"text_offset\")] public IReadOnlyList<int> TextOffset { get; init; } Property Value Type Description IReadOnlyList<int> | Improve this Doc View Source TokenLogProbability Log Probability of Tokens. Declaration [JsonPropertyName(\"token_logprobs\")] public IReadOnlyList<float?> TokenLogProbability { get; init; } Property Value Type Description IReadOnlyList<float?> | Improve this Doc View Source Tokens Tokens. Declaration [JsonPropertyName(\"tokens\")] public IReadOnlyList<string> Tokens { get; init; } Property Value Type Description IReadOnlyList<string> | Improve this Doc View Source TopLogProbability Top Log Probabilities. Declaration [JsonPropertyName(\"top_logprobs\")] public IReadOnlyList<IDictionary<string, float>> TopLogProbability { get; init; } Property Value Type Description IReadOnlyList<IDictionary<string, float>> Implements System.IEquatable<T>"
  },
  "api/DotnetPrompt.LLM.OpenAI.Model.CompletionsUsage.html": {
    "href": "api/DotnetPrompt.LLM.OpenAI.Model.CompletionsUsage.html",
    "title": "Class CompletionsUsage | DotnetPrompt",
    "keywords": "Class CompletionsUsage Representation of the token counts processed for a completions request. Counts consider all tokens across prompts, choices, choice alternates, best_of generations, and other consumers. Inheritance object CompletionsUsage Implements IEquatable<CompletionsUsage> Inherited Members object.Equals(object) object.Equals(object, object) object.GetHashCode() object.GetType() object.MemberwiseClone() object.ReferenceEquals(object, object) object.ToString() Namespace: DotnetPrompt.LLM.OpenAI.Model Assembly: DotnetPrompt.LLM.OpenAI.dll Syntax public record CompletionsUsage : IEquatable<CompletionsUsage> Properties | Improve this Doc View Source CompletionTokens Number of tokens received in the completion. Declaration [JsonPropertyName(\"completion_tokens\")] public int CompletionTokens { get; init; } Property Value Type Description int | Improve this Doc View Source PromptTokens Number of tokens sent in the original request. Declaration [JsonPropertyName(\"prompt_tokens\")] public int PromptTokens { get; init; } Property Value Type Description int | Improve this Doc View Source TotalTokens Total number of tokens transacted in this request/response. Declaration [JsonPropertyName(\"total_tokens\")] public int TotalTokens { get; init; } Property Value Type Description int Implements System.IEquatable<T>"
  },
  "api/DotnetPrompt.LLM.OpenAI.Model.EmbeddingItem.html": {
    "href": "api/DotnetPrompt.LLM.OpenAI.Model.EmbeddingItem.html",
    "title": "Class EmbeddingItem | DotnetPrompt",
    "keywords": "Class EmbeddingItem Expected response schema to embeddings object list item request. Inheritance object EmbeddingItem Implements IEquatable<EmbeddingItem> Inherited Members object.Equals(object) object.Equals(object, object) object.GetHashCode() object.GetType() object.MemberwiseClone() object.ReferenceEquals(object, object) object.ToString() Namespace: DotnetPrompt.LLM.OpenAI.Model Assembly: DotnetPrompt.LLM.OpenAI.dll Syntax public record EmbeddingItem : IEquatable<EmbeddingItem> Properties | Improve this Doc View Source Embedding List of embeddings value for the input prompt. These represents a measurement of releated of text strings. Declaration [JsonPropertyName(\"embedding\")] public IReadOnlyList<float> Embedding { get; init; } Property Value Type Description IReadOnlyList<float> | Improve this Doc View Source Index Index of the prompt to which the EmbeddingItem corresponds. Declaration [JsonPropertyName(\"index\")] public int Index { get; init; } Property Value Type Description int | Improve this Doc View Source Text Text of embeddings Declaration [JsonIgnore] public string Text { get; set; } Property Value Type Description string Implements System.IEquatable<T>"
  },
  "api/DotnetPrompt.LLM.OpenAI.Model.Embeddings.html": {
    "href": "api/DotnetPrompt.LLM.OpenAI.Model.Embeddings.html",
    "title": "Class Embeddings | DotnetPrompt",
    "keywords": "Class Embeddings Inheritance object Embeddings Implements IEquatable<Embeddings> Inherited Members object.Equals(object) object.Equals(object, object) object.GetHashCode() object.GetType() object.MemberwiseClone() object.ReferenceEquals(object, object) object.ToString() Namespace: DotnetPrompt.LLM.OpenAI.Model Assembly: DotnetPrompt.LLM.OpenAI.dll Syntax public record Embeddings : IEquatable<Embeddings> Properties | Improve this Doc View Source Data Embedding values for the prompts submitted in the request. Declaration [JsonPropertyName(\"data\")] public IReadOnlyList<EmbeddingItem> Data { get; init; } Property Value Type Description IReadOnlyList<EmbeddingItem> | Improve this Doc View Source Model ID of the model to use. Declaration [JsonPropertyName(\"model\")] public string Model { get; init; } Property Value Type Description string | Improve this Doc View Source Usage Usage counts for tokens input using the embeddings API. Declaration [JsonPropertyName(\"usage\")] public EmbeddingsUsage Usage { get; init; } Property Value Type Description EmbeddingsUsage Implements System.IEquatable<T>"
  },
  "api/DotnetPrompt.LLM.OpenAI.Model.EmbeddingsUsage.html": {
    "href": "api/DotnetPrompt.LLM.OpenAI.Model.EmbeddingsUsage.html",
    "title": "Class EmbeddingsUsage | DotnetPrompt",
    "keywords": "Class EmbeddingsUsage Measurment of the amount of tokens used in this request and response. Inheritance object EmbeddingsUsage Implements IEquatable<EmbeddingsUsage> Inherited Members object.Equals(object) object.Equals(object, object) object.GetHashCode() object.GetType() object.MemberwiseClone() object.ReferenceEquals(object, object) object.ToString() Namespace: DotnetPrompt.LLM.OpenAI.Model Assembly: DotnetPrompt.LLM.OpenAI.dll Syntax public record EmbeddingsUsage : IEquatable<EmbeddingsUsage> Properties | Improve this Doc View Source PromptTokens Number of tokens sent in the original request. Declaration [JsonPropertyName(\"prompt_tokens\")] public int PromptTokens { get; init; } Property Value Type Description int | Improve this Doc View Source TotalTokens Total number of tokens transacted in this request/response. Declaration [JsonPropertyName(\"total_tokens\")] public int TotalTokens { get; init; } Property Value Type Description int Implements System.IEquatable<T>"
  },
  "api/DotnetPrompt.LLM.OpenAI.Model.html": {
    "href": "api/DotnetPrompt.LLM.OpenAI.Model.html",
    "title": "Namespace DotnetPrompt.LLM.OpenAI.Model | DotnetPrompt",
    "keywords": "Namespace DotnetPrompt.LLM.OpenAI.Model Classes ChatMessage Choice Completions Expected response schema to completion request. CompletionsLogProbability CompletionsUsage Representation of the token counts processed for a completions request. Counts consider all tokens across prompts, choices, choice alternates, best_of generations, and other consumers. EmbeddingItem Expected response schema to embeddings object list item request. Embeddings EmbeddingsUsage Measurment of the amount of tokens used in this request and response."
  },
  "api/DotnetPrompt.LLM.OpenAI.OpenAIEmbeddings.html": {
    "href": "api/DotnetPrompt.LLM.OpenAI.OpenAIEmbeddings.html",
    "title": "Class OpenAIEmbeddings | DotnetPrompt",
    "keywords": "Class OpenAIEmbeddings Embeddings client for OpenAI Inheritance object OpenAIEmbeddings Implements IEmbeddings Inherited Members object.Equals(object) object.Equals(object, object) object.GetHashCode() object.GetType() object.MemberwiseClone() object.ReferenceEquals(object, object) object.ToString() Namespace: DotnetPrompt.LLM.OpenAI Assembly: DotnetPrompt.LLM.OpenAI.dll Syntax public class OpenAIEmbeddings : IEmbeddings Constructors | Improve this Doc View Source OpenAIEmbeddings(IConfiguration) Declaration public OpenAIEmbeddings(IConfiguration configuration) Parameters Type Name Description IConfiguration configuration | Improve this Doc View Source OpenAIEmbeddings(string) Default constructor Declaration public OpenAIEmbeddings(string openAiKey) Parameters Type Name Description string openAiKey Properties | Improve this Doc View Source BatchSize Size of the batch Declaration public int BatchSize { get; init; } Property Value Type Description int Remarks Default is 20 | Improve this Doc View Source MaxTokens Declaration public int MaxTokens { get; init; } Property Value Type Description int Methods | Improve this Doc View Source EmbedAsync(IList<string>) Call out to OpenAI's embedding endpoint for embedding search docs. Declaration public Task<IList<TextEmbedding>> EmbedAsync(IList<string> texts) Parameters Type Name Description IList<string> texts Returns Type Description Task<IList<TextEmbedding>> | Improve this Doc View Source EmbedAsync(string) Call OpenAI embeddings to embed a single text Declaration public Task<TextEmbedding> EmbedAsync(string text) Parameters Type Name Description string text Returns Type Description Task<TextEmbedding> Implements IEmbeddings"
  },
  "api/DotnetPrompt.LLM.OpenAI.OpenAIModel.html": {
    "href": "api/DotnetPrompt.LLM.OpenAI.OpenAIModel.html",
    "title": "Class OpenAIModel | DotnetPrompt",
    "keywords": "Class OpenAIModel Model for OpenAI Inheritance object BaseModel OpenAIModel AzureOpenAIModel Implements ILargeLanguageModel Inherited Members BaseModel.Logger BaseModel.DefaultStop BaseModel.UseCache BaseModel.GenerateAsync(IList<string>, IList<string>) object.Equals(object) object.Equals(object, object) object.GetHashCode() object.GetType() object.MemberwiseClone() object.ReferenceEquals(object, object) object.ToString() Namespace: DotnetPrompt.LLM.OpenAI Assembly: DotnetPrompt.LLM.OpenAI.dll Syntax public class OpenAIModel : BaseModel, ILargeLanguageModel Constructors | Improve this Doc View Source OpenAIModel(IConfiguration, ILogger<OpenAIModel>, IDistributedCache) Constructor to use in DI Declaration public OpenAIModel(IConfiguration configuration, ILogger<OpenAIModel> logger, IDistributedCache cache) Parameters Type Name Description IConfiguration configuration ILogger<OpenAIModel> logger IDistributedCache cache Exceptions Type Condition System.InvalidOperationException | Improve this Doc View Source OpenAIModel(string, OpenAIModelConfiguration, ILogger, IDistributedCache) Declaration public OpenAIModel(string openAIApiKey, OpenAIModelConfiguration defaultModelConfiguration, ILogger logger, IDistributedCache cache = null) Parameters Type Name Description string openAIApiKey OpenAIModelConfiguration defaultModelConfiguration ILogger logger IDistributedCache cache | Improve this Doc View Source OpenAIModel(string, OpenAIModelConfiguration) Default constructor without logging Declaration public OpenAIModel(string openAIApiKey, OpenAIModelConfiguration defaultModelConfiguration) Parameters Type Name Description string openAIApiKey OpenAIModelConfiguration defaultModelConfiguration Fields | Improve this Doc View Source BatchSize Declaration public int BatchSize Field Value Type Description int | Improve this Doc View Source OpenAiApiKey Declaration public string OpenAiApiKey Field Value Type Description string Properties | Improve this Doc View Source DefaultModelConfiguration Declaration public OpenAIModelConfiguration DefaultModelConfiguration { get; init; } Property Value Type Description OpenAIModelConfiguration | Improve this Doc View Source LLMType Declaration public override string LLMType { get; } Property Value Type Description string Overrides BaseModel.LLMType | Improve this Doc View Source MaxRequestTokens Declaration public override int MaxRequestTokens { get; } Property Value Type Description int Overrides BaseModel.MaxRequestTokens | Improve this Doc View Source ModelExtraArguments Holds any model parameters valid for create call not explicitly specified. Declaration public Dictionary<string, object> ModelExtraArguments { get; set; } Property Value Type Description Dictionary<string, object> | Improve this Doc View Source Streaming Declaration public bool Streaming { get; init; } Property Value Type Description bool Methods | Improve this Doc View Source AsUniqueString() Declaration protected override string AsUniqueString() Returns Type Description string Overrides BaseModel.AsUniqueString() | Improve this Doc View Source CompletionsResponseValue(OpenAIModelConfiguration) Declaration protected virtual Task<Completions> CompletionsResponseValue(OpenAIModelConfiguration options) Parameters Type Name Description OpenAIModelConfiguration options Returns Type Description Task<Completions> | Improve this Doc View Source CompletionWithRetry(OpenAIModelConfiguration) Use tenacity to retry the completion call. Declaration public Task<Completions> CompletionWithRetry(OpenAIModelConfiguration modelConfiguration) Parameters Type Name Description OpenAIModelConfiguration modelConfiguration Returns Type Description Task<Completions> | Improve this Doc View Source GenerateInternalAsync(IList<string>, IList<string>) Declaration protected override Task<ModelResult> GenerateInternalAsync(IList<string> prompts, IList<string> stop = null) Parameters Type Name Description IList<string> prompts IList<string> stop Returns Type Description Task<ModelResult> Overrides BaseModel.GenerateInternalAsync(IList<string>, IList<string>) | Improve this Doc View Source GetNumTokens(string) Declaration public override int GetNumTokens(string text) Parameters Type Name Description string text Returns Type Description int Overrides BaseModel.GetNumTokens(string) | Improve this Doc View Source GetSubPrompts(OpenAIModelConfiguration, IList<string>) Declaration public List<List<string>> GetSubPrompts(OpenAIModelConfiguration completionsOptions, IList<string> prompts) Parameters Type Name Description OpenAIModelConfiguration completionsOptions IList<string> prompts Returns Type Description List<List<string>> | Improve this Doc View Source MaxTokensForPrompt(string) Calculate the maximum number of tokens possible to generate for a prompt. Declaration public int MaxTokensForPrompt(string prompt) Parameters Type Name Description string prompt The prompt to pass into the model. Returns Type Description int The maximum number of tokens to generate for a prompt. Examples var maxTokens = openai.MaxTokensForPrompt(\"Tell me a joke.\") | Improve this Doc View Source ModelNameToContextSize(string) Calculate the maximum number of tokens possible to generate for a model. text-davinci-003: 4,097 tokens text-curie-001: 2,048 tokens text-babbage-001: 2,048 tokens text-ada-001: 2,048 tokens code-davinci-002: 8,000 tokens code-cushman-001: 2,048 tokens Declaration public int ModelNameToContextSize(string modelName) Parameters Type Name Description string modelName The modelname we want to know the context size for. Returns Type Description int The maximum context size Implements ILargeLanguageModel"
  },
  "api/DotnetPrompt.LLM.OpenAI.OpenAIModelConfiguration.html": {
    "href": "api/DotnetPrompt.LLM.OpenAI.OpenAIModelConfiguration.html",
    "title": "Class OpenAIModelConfiguration | DotnetPrompt",
    "keywords": "Class OpenAIModelConfiguration Inheritance object OpenAIModelConfiguration ChatGptModelConfiguration Implements IEquatable<OpenAIModelConfiguration> Inherited Members object.Equals(object) object.Equals(object, object) object.GetHashCode() object.GetType() object.MemberwiseClone() object.ReferenceEquals(object, object) object.ToString() Namespace: DotnetPrompt.LLM.OpenAI Assembly: DotnetPrompt.LLM.OpenAI.dll Syntax public record OpenAIModelConfiguration : IEquatable<OpenAIModelConfiguration> Fields | Improve this Doc View Source Default Default model configuration Declaration public static OpenAIModelConfiguration Default Field Value Type Description OpenAIModelConfiguration Properties | Improve this Doc View Source CacheLevel can be used to disable any server-side caching, 0=no cache, 1=prompt prefix enabled, 2=full cache Declaration [JsonPropertyName(\"cache_level\")] public int? CacheLevel { get; init; } Property Value Type Description int? | Improve this Doc View Source CompletionConfig Completion configuration. Declaration [JsonPropertyName(\"completion_config\")] public string CompletionConfig { get; init; } Property Value Type Description string | Improve this Doc View Source Echo Echo back the prompt in addition to the completion. Declaration [JsonPropertyName(\"echo\")] public bool? Echo { get; init; } Property Value Type Description bool? | Improve this Doc View Source FrequencyPenalty How much to penalize new tokens based on whether they appear in the text so far. Increases the model's likelihood to talk about new topics. Declaration [JsonPropertyName(\"frequency_penalty\")] public float? FrequencyPenalty { get; init; } Property Value Type Description float? | Improve this Doc View Source GenerationSampleCount How many generations to create server side, and display only the best. Will not stream intermediate progress if best_of > 1. Has maximum value of 128. Declaration [JsonPropertyName(\"best_of\")] public int? GenerationSampleCount { get; init; } Property Value Type Description int? | Improve this Doc View Source LogitBias Defaults to null. Modify the likelihood of specified tokens appearing in the completion. Accepts a json object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this tokenizer tool (which works for both GPT-2 and GPT-3) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. As an example, you can pass {\"50256\" &#58; -100} to prevent the <|endoftext|> token from being generated. Declaration [JsonPropertyName(\"logit_bias\")] public IDictionary<string, int> LogitBias { get; init; } Property Value Type Description IDictionary<string, int> | Improve this Doc View Source LogProbability Include the log probabilities on the logprobs most likely tokens, as well the chosen tokens. So for example, if logprobs is 10, the API will return a list of the 10 most likely tokens. If logprobs is 0, only the chosen tokens will have logprobs returned. Minimum of 0 and maximum of 100 allowed. Declaration [JsonPropertyName(\"logprobs\")] public int? LogProbability { get; init; } Property Value Type Description int? | Improve this Doc View Source MaxTokens The maximum number of tokens to generate. Has minimum of 0. Declaration [JsonPropertyName(\"max_tokens\")] public int? MaxTokens { get; set; } Property Value Type Description int? Remarks Could be set to -1 if there a single prompt to return as many tokens as possible given the prompt and the models maximal context size. | Improve this Doc View Source Model The name of the model to use. Declaration [JsonPropertyName(\"model\")] public string Model { get; init; } Property Value Type Description string | Improve this Doc View Source NucleusSamplingFactor An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend using this or temperature but not both. Minimum of 0 and maximum of 1 allowed. Declaration [JsonPropertyName(\"top_p\")] public float? NucleusSamplingFactor { get; init; } Property Value Type Description float? | Improve this Doc View Source PresencePenalty How much to penalize new tokens based on their existing frequency in the text so far. Decreases the model's likelihood to repeat the same line verbatim. Has minimum of -2 and maximum of 2. Declaration [JsonPropertyName(\"presence_penalty\")] public float? PresencePenalty { get; init; } Property Value Type Description float? | Improve this Doc View Source Prompt An optional prompt to complete from, encoded as a string, a list of strings, or a list of token lists. Defaults to <|endoftext|>. The prompt to complete from. If you would like to provide multiple prompts, use the POST variant of this method. Note that <|endoftext|> is the document separator that the model sees during training, so if a prompt is not specified the model will generate as if from the beginning of a new document. Maximum allowed size of string list is 2048. Declaration [JsonPropertyName(\"prompt\")] public IList<string> Prompt { get; init; } Property Value Type Description IList<string> | Improve this Doc View Source SnippetCount How many snippets to generate for each prompt. Minimum of 1 and maximum of 128 allowed. Declaration [JsonPropertyName(\"n\")] public int? SnippetCount { get; init; } Property Value Type Description int? | Improve this Doc View Source Stop A sequence which indicates the end of the current document. Declaration [JsonPropertyName(\"stop\")] public IList<string> Stop { get; set; } Property Value Type Description IList<string> | Improve this Doc View Source Temperature What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer. We generally recommend using this or top_p but not both. Minimum of 0 and maximum of 2 allowed. Declaration [JsonPropertyName(\"temperature\")] public float? Temperature { get; init; } Property Value Type Description float? | Improve this Doc View Source User The ID of the end-user, for use in tracking and rate-limiting. Declaration [JsonPropertyName(\"user\")] public string User { get; init; } Property Value Type Description string Implements System.IEquatable<T>"
  },
  "api/DotnetPrompt.LLM.OpenAI.Tokens.html": {
    "href": "api/DotnetPrompt.LLM.OpenAI.Tokens.html",
    "title": "Class Tokens | DotnetPrompt",
    "keywords": "Class Tokens Inheritance object Tokens Inherited Members object.Equals(object) object.Equals(object, object) object.GetHashCode() object.GetType() object.MemberwiseClone() object.ReferenceEquals(object, object) object.ToString() Namespace: DotnetPrompt.LLM.OpenAI Assembly: DotnetPrompt.LLM.OpenAI.dll Syntax public static class Tokens Fields | Improve this Doc View Source ImEnd Declaration public const string ImEnd = \"<|im_end|>\" Field Value Type Description string | Improve this Doc View Source ImStart Declaration public const string ImStart = \"<|im_start|>\" Field Value Type Description string"
  },
  "api/DotnetPrompt.Prompts.ChatMessageTemplate.html": {
    "href": "api/DotnetPrompt.Prompts.ChatMessageTemplate.html",
    "title": "Class ChatMessageTemplate | DotnetPrompt",
    "keywords": "Class ChatMessageTemplate Prompt template of chat message Inheritance object ChatMessageTemplate Implements IEquatable<ChatMessageTemplate> Inherited Members object.Equals(object) object.Equals(object, object) object.GetHashCode() object.GetType() object.MemberwiseClone() object.ReferenceEquals(object, object) object.ToString() Namespace: DotnetPrompt.Prompts Assembly: DotnetPrompt.dll Syntax public record ChatMessageTemplate : IEquatable<ChatMessageTemplate> Constructors | Improve this Doc View Source ChatMessageTemplate(string, IPromptTemplate) Prompt template of chat message Declaration public ChatMessageTemplate(string Role, IPromptTemplate ContentTemplate) Parameters Type Name Description string Role One of the chat role IPromptTemplate ContentTemplate Template for content, might be parameterless Properties | Improve this Doc View Source ContentTemplate Template for content, might be parameterless Declaration public IPromptTemplate ContentTemplate { get; init; } Property Value Type Description IPromptTemplate | Improve this Doc View Source Role One of the chat role Declaration public string Role { get; init; } Property Value Type Description string Implements System.IEquatable<T>"
  },
  "api/DotnetPrompt.Prompts.ChatMLPromptTemplate.html": {
    "href": "api/DotnetPrompt.Prompts.ChatMLPromptTemplate.html",
    "title": "Class ChatMLPromptTemplate | DotnetPrompt",
    "keywords": "Class ChatMLPromptTemplate PromptTemplate for generating ChatML messages Inheritance object ChatMLPromptTemplate Implements IPromptTemplate Inherited Members object.Equals(object) object.Equals(object, object) object.GetHashCode() object.GetType() object.MemberwiseClone() object.ReferenceEquals(object, object) object.ToString() Namespace: DotnetPrompt.Prompts Assembly: DotnetPrompt.dll Syntax public class ChatMLPromptTemplate : IPromptTemplate Constructors | Improve this Doc View Source ChatMLPromptTemplate(IPromptTemplate, string?, IList<(string userMessage, string assistantMessage)>?) ctor Declaration public ChatMLPromptTemplate(IPromptTemplate content, string? system = null, IList<(string userMessage, string assistantMessage)>? priorMessages = null) Parameters Type Name Description IPromptTemplate content PromptTemplate for content string string system System string IList<(string userMessage, string assistantMessage)> priorMessages | Improve this Doc View Source ChatMLPromptTemplate(string, string?, IList<(string userMessage, string assistantMessage)>?) ctor Declaration public ChatMLPromptTemplate(string content, string? system = null, IList<(string userMessage, string assistantMessage)>? priorMessages = null) Parameters Type Name Description string content Content string string system System string IList<(string userMessage, string assistantMessage)> priorMessages Properties | Improve this Doc View Source InputVariables A list of the names of the variables the prompt template expects. Declaration public IList<string> InputVariables { get; } Property Value Type Description IList<string> | Improve this Doc View Source Messages List of ChatMessageTemplate Declaration public IList<ChatMessageTemplate> Messages { get; } Property Value Type Description IList<ChatMessageTemplate> Methods | Improve this Doc View Source AddPriorResponse(string, string) Add prior response pair Declaration public ChatMLPromptTemplate AddPriorResponse(string userMessage, string assistantMessage) Parameters Type Name Description string userMessage string assistantMessage Returns Type Description ChatMLPromptTemplate | Improve this Doc View Source AddSystemMessage(string) Add system message Declaration public ChatMLPromptTemplate AddSystemMessage(string systemMessage) Parameters Type Name Description string systemMessage Returns Type Description ChatMLPromptTemplate | Improve this Doc View Source Format(IDictionary<string, string>?) Build a prompt from current template and a list of values. Declaration public string Format(IDictionary<string, string>? values = null) Parameters Type Name Description IDictionary<string, string> values Key-Value list of values to use to build prompt. Returns Type Description string String prompt Exceptions Type Condition System.ArgumentException Throws when provided list of keys does not match InputVariables. Implements IPromptTemplate"
  },
  "api/DotnetPrompt.Prompts.ChatRoles.html": {
    "href": "api/DotnetPrompt.Prompts.ChatRoles.html",
    "title": "Class ChatRoles | DotnetPrompt",
    "keywords": "Class ChatRoles List of chatml roles Inheritance object ChatRoles Inherited Members object.Equals(object) object.Equals(object, object) object.GetHashCode() object.GetType() object.MemberwiseClone() object.ReferenceEquals(object, object) object.ToString() Namespace: DotnetPrompt.Prompts Assembly: DotnetPrompt.dll Syntax public static class ChatRoles Fields | Improve this Doc View Source Assistant Assistant Role Declaration public const string Assistant = \"assistant\" Field Value Type Description string | Improve this Doc View Source System System Role Declaration public const string System = \"system\" Field Value Type Description string | Improve this Doc View Source User User Role Declaration public const string User = \"user\" Field Value Type Description string"
  },
  "api/DotnetPrompt.Prompts.ExampleSelectors.html": {
    "href": "api/DotnetPrompt.Prompts.ExampleSelectors.html",
    "title": "Namespace DotnetPrompt.Prompts.ExampleSelectors | DotnetPrompt",
    "keywords": "Namespace DotnetPrompt.Prompts.ExampleSelectors Classes LengthBasedExampleSelector Select examples for FewShotPromptTemplate based on length. Interfaces IExampleSelector Interface for selecting examples to include in prompts."
  },
  "api/DotnetPrompt.Prompts.ExampleSelectors.IExampleSelector.html": {
    "href": "api/DotnetPrompt.Prompts.ExampleSelectors.IExampleSelector.html",
    "title": "Interface IExampleSelector | DotnetPrompt",
    "keywords": "Interface IExampleSelector Interface for selecting examples to include in prompts. Namespace: DotnetPrompt.Prompts.ExampleSelectors Assembly: DotnetPrompt.dll Syntax public interface IExampleSelector Methods | Improve this Doc View Source AddExample(IDictionary<string, string>) Add new example to store for a key. Declaration void AddExample(IDictionary<string, string> example) Parameters Type Name Description IDictionary<string, string> example List of examples to select. Remarks Example should be a dictionary with the keys being the input variables and the values being the values for those input variables. | Improve this Doc View Source SelectExamples(IDictionary<string, string>) Select which examples to use based on the inputs. Declaration IList<IDictionary<string, string>> SelectExamples(IDictionary<string, string> inputVariables) Parameters Type Name Description IDictionary<string, string> inputVariables List of input variables that should be user for calculation of possible examples Returns Type Description IList<IDictionary<string, string>>"
  },
  "api/DotnetPrompt.Prompts.ExampleSelectors.LengthBasedExampleSelector.html": {
    "href": "api/DotnetPrompt.Prompts.ExampleSelectors.LengthBasedExampleSelector.html",
    "title": "Class LengthBasedExampleSelector | DotnetPrompt",
    "keywords": "Class LengthBasedExampleSelector Select examples for FewShotPromptTemplate based on length. Inheritance object LengthBasedExampleSelector Implements IExampleSelector Inherited Members object.Equals(object) object.Equals(object, object) object.GetHashCode() object.GetType() object.MemberwiseClone() object.ReferenceEquals(object, object) object.ToString() Namespace: DotnetPrompt.Prompts.ExampleSelectors Assembly: DotnetPrompt.dll Syntax public class LengthBasedExampleSelector : IExampleSelector Constructors | Improve this Doc View Source LengthBasedExampleSelector(IList<IDictionary<string, string>>, IPromptTemplate) Declaration public LengthBasedExampleSelector(IList<IDictionary<string, string>> examples, IPromptTemplate examplePrompt) Parameters Type Name Description IList<IDictionary<string, string>> examples A list of the examples that the prompt template expects. IPromptTemplate examplePrompt Prompt template used to format the examples. Properties | Improve this Doc View Source GetTextLength Function to measure prompt length. Defaults to word count. Declaration public Func<string, int> GetTextLength { get; init; } Property Value Type Description Func<string, int> | Improve this Doc View Source MaxLength Max length for the prompt, beyond which examples are cut. Declaration public int MaxLength { get; init; } Property Value Type Description int Methods | Improve this Doc View Source AddExample(IDictionary<string, string>) Add new example to store for a key. Declaration public void AddExample(IDictionary<string, string> example) Parameters Type Name Description IDictionary<string, string> example List of examples to select. Remarks Example should be a dictionary with the keys being the input variables and the values being the values for those input variables. | Improve this Doc View Source SelectExamples(IDictionary<string, string>) Select which examples to use based on the inputs. Declaration public IList<IDictionary<string, string>> SelectExamples(IDictionary<string, string> inputVariables) Parameters Type Name Description IDictionary<string, string> inputVariables List of input variables that should be user for calculation of possible examples Returns Type Description IList<IDictionary<string, string>> Implements IExampleSelector"
  },
  "api/DotnetPrompt.Prompts.FewShotPromptTemplate.html": {
    "href": "api/DotnetPrompt.Prompts.FewShotPromptTemplate.html",
    "title": "Class FewShotPromptTemplate | DotnetPrompt",
    "keywords": "Class FewShotPromptTemplate The FewShotPromptTemplate class allows developers to create prompts for natural language processing (NLP) models using few-shot learning techniques. This class takes in a prefix, examples, and suffix, and combines them with a separator to generate a prompt for the language model. Inheritance object FewShotPromptTemplate Implements IPromptTemplate Inherited Members object.Equals(object) object.Equals(object, object) object.GetHashCode() object.GetType() object.MemberwiseClone() object.ReferenceEquals(object, object) object.ToString() Namespace: DotnetPrompt.Prompts Assembly: DotnetPrompt.dll Syntax public class FewShotPromptTemplate : IPromptTemplate Constructors | Improve this Doc View Source FewShotPromptTemplate(IPromptTemplate, IPromptTemplate, IPromptTemplate, IExampleSelector) Constructor with example selector and prefix Declaration public FewShotPromptTemplate(IPromptTemplate prefixPromptTemplate, IPromptTemplate examplePromptTemplate, IPromptTemplate suffixPromptTemplate, IExampleSelector exampleSelector) Parameters Type Name Description IPromptTemplate prefixPromptTemplate PromptTemplate to format prefix IPromptTemplate examplePromptTemplate PromptTemplate to format Example IPromptTemplate suffixPromptTemplate PromptTemplate to format suffix IExampleSelector exampleSelector Example selector that will select examples for prompt | Improve this Doc View Source FewShotPromptTemplate(IPromptTemplate, IPromptTemplate, IPromptTemplate, IList<IDictionary<string, string>>) Constructor with examples and prefix Declaration public FewShotPromptTemplate(IPromptTemplate prefixPromptTemplate, IPromptTemplate examplePromptTemplate, IPromptTemplate suffixPromptTemplate, IList<IDictionary<string, string>> examples) Parameters Type Name Description IPromptTemplate prefixPromptTemplate PromptTemplate to format prefix IPromptTemplate examplePromptTemplate PromptTemplate to format Example IPromptTemplate suffixPromptTemplate PromptTemplate to format suffix IList<IDictionary<string, string>> examples List of Examples | Improve this Doc View Source FewShotPromptTemplate(IPromptTemplate, IPromptTemplate, IExampleSelector) Constructor with example selector Declaration public FewShotPromptTemplate(IPromptTemplate examplePromptTemplate, IPromptTemplate suffixPromptTemplate, IExampleSelector exampleSelector) Parameters Type Name Description IPromptTemplate examplePromptTemplate PromptTemplate to format Example IPromptTemplate suffixPromptTemplate PromptTemplate to format suffix IExampleSelector exampleSelector Example selector that will select examples for prompt | Improve this Doc View Source FewShotPromptTemplate(IPromptTemplate, IPromptTemplate, IList<IDictionary<string, string>>) Constructor with examples Declaration public FewShotPromptTemplate(IPromptTemplate examplePromptTemplate, IPromptTemplate suffixPromptTemplate, IList<IDictionary<string, string>> examples) Parameters Type Name Description IPromptTemplate examplePromptTemplate PromptTemplate to format Example IPromptTemplate suffixPromptTemplate PromptTemplate to format suffix IList<IDictionary<string, string>> examples List of Examples Properties | Improve this Doc View Source Examples List of Examples, where each example is a Dictionary where key is Input Variable and value is Input Value Declaration public IList<IDictionary<string, string>>? Examples { get; init; } Property Value Type Description IList<IDictionary<string, string>> | Improve this Doc View Source ExampleSeparator Separator for examples, default value is \"\\n\\n\" Declaration public string ExampleSeparator { get; set; } Property Value Type Description string | Improve this Doc View Source InputVariables A list of the names of the variables the prompt template expects. Declaration public IList<string> InputVariables { get; init; } Property Value Type Description IList<string> Methods | Improve this Doc View Source Format(IDictionary<string, string>?) Build a prompt from current template and a list of values. Declaration public string Format(IDictionary<string, string>? values = null) Parameters Type Name Description IDictionary<string, string> values Key-Value list of values to use to build prompt. Returns Type Description string String prompt Exceptions Type Condition System.ArgumentException Throws when provided list of keys does not match InputVariables. Implements IPromptTemplate"
  },
  "api/DotnetPrompt.Prompts.html": {
    "href": "api/DotnetPrompt.Prompts.html",
    "title": "Namespace DotnetPrompt.Prompts | DotnetPrompt",
    "keywords": "Namespace DotnetPrompt.Prompts Classes ChatMessageTemplate Prompt template of chat message ChatMLPromptTemplate PromptTemplate for generating ChatML messages ChatRoles List of chatml roles FewShotPromptTemplate The FewShotPromptTemplate class allows developers to create prompts for natural language processing (NLP) models using few-shot learning techniques. This class takes in a prefix, examples, and suffix, and combines them with a separator to generate a prompt for the language model. PromptTemplate Schema to represent a prompt for an LLM based on formatted string"
  },
  "api/DotnetPrompt.Prompts.PromptTemplate.html": {
    "href": "api/DotnetPrompt.Prompts.PromptTemplate.html",
    "title": "Class PromptTemplate | DotnetPrompt",
    "keywords": "Class PromptTemplate Schema to represent a prompt for an LLM based on formatted string Inheritance object PromptTemplate Implements IPromptTemplate Inherited Members object.Equals(object) object.Equals(object, object) object.GetHashCode() object.GetType() object.MemberwiseClone() object.ReferenceEquals(object, object) object.ToString() Namespace: DotnetPrompt.Prompts Assembly: DotnetPrompt.dll Syntax public class PromptTemplate : IPromptTemplate Constructors | Improve this Doc View Source PromptTemplate(string, IList<string>) Build PromptTemplate Schema from template string and inputVariables Declaration public PromptTemplate(string template, IList<string> inputVariables) Parameters Type Name Description string template Template string IList<string> inputVariables List of input variables Exceptions Type Condition System.ArgumentException Throws when list of input variables is invalid | Improve this Doc View Source PromptTemplate(string) Build PromptTemplate Schema from template string Declaration public PromptTemplate(string template) Parameters Type Name Description string template Properties | Improve this Doc View Source InputVariables A list of the names of the variables the prompt template expects. Declaration public IList<string> InputVariables { get; set; } Property Value Type Description IList<string> | Improve this Doc View Source Template The prompt template. Declaration public string Template { get; set; } Property Value Type Description string Methods | Improve this Doc View Source Format(IDictionary<string, string>?) Build a prompt from current template and a list of values. Declaration public string Format(IDictionary<string, string>? values = null) Parameters Type Name Description IDictionary<string, string> values Key-Value list of values to use to build prompt. Returns Type Description string String prompt Exceptions Type Condition System.ArgumentException Throws when provided list of keys does not match InputVariables. | Improve this Doc View Source Format((string, string)[]) Build a prompt from current template and a list of value tuples. Declaration public string Format((string, string)[] values) Parameters Type Name Description (string, string)[] values Tuple of key - value Returns Type Description string | Improve this Doc View Source FromExamples(List<string>, string, List<string>, string, string) Take examples in list format with prefix and suffix to create a prompt. Intended be used as a way to dynamically create a prompt from examples Declaration public static PromptTemplate FromExamples(List<string> examples, string suffix, List<string> inputVariables, string prefix = \"\", string exampleSeparator = \"\\n\\n\") Parameters Type Name Description List<string> examples List of examples to use in the prompt. string suffix String to go after the list of examples. Should generally set up the user's input. List<string> inputVariables A list of variable names the final prompt template will expect. string prefix String that should go before any examples. Generally includes examples. Default to an empty string string exampleSeparator The separator to use in between examples. Defaults to two new line characters. Returns Type Description PromptTemplate The final prompt generated. Remarks Verbatum string on windows will produce \\r\\n and that need to be handled somehow (todo). | Improve this Doc View Source FromFile(string, string[]) Load a prompt from a file. Declaration public static PromptTemplate FromFile(string templateFile, string[] inputVariables) Parameters Type Name Description string templateFile The path to the file containing the prompt template. string[] inputVariables A list of variable names the final prompt template will expect. Returns Type Description PromptTemplate The prompt loaded from the file. Implements IPromptTemplate"
  },
  "api/index.html": {
    "href": "api/index.html",
    "title": "Components | DotnetPrompt",
    "keywords": "Components This are references to primary DotnetPrompt components. Prompts Abstractions.Propmpts Prompts Large Language Models Abstractions.LLM OpenAI Chains Abstractions.Chains Chains"
  },
  "articles/chains/getting_started.html": {
    "href": "articles/chains/getting_started.html",
    "title": "Getting Started | DotnetPrompt",
    "keywords": "Getting Started Using an LLM in isolation is fine for some simple applications, but many more complex ones require chaining LLMs - either with each other or with other components. DotnetPrompt provides a standard interface IChains, as well as some common implementations of chains for ease of use. Why do we need chains? Chaining multiple LLM runs together (with the output of one step being the input to the next) can help users accomplish more complex tasks, and in a way that is perceived to be more transparent and controllable. Query an LLM with the ModelChain The ModelChain is a simple chain that takes in a prompt template, formats it with the user input and returns the response from an LLM. To use the ModelChain, first create a prompt template. var llm = new OpenAIModel(Constants.OpenAIKey, OpenAIModelConfiguration.Default with { Temperature = 0.9f }); var prompt = new PromptTemplate(\"What is a good name for a company that makes {product}?\"); We can now create a very simple chain. To run the chain and get a result back we could use extension method PromptAsync. var chain = new ModelChain(prompt, llm); var executor = chain.GetExecutor(); // Run the chain only specifying the input variable. var result = await executor.PromptAsync(\"colorful socks\"); Console.WriteLine(result); > CheeryToes Sockery. This is one of the simpler types of chains, but understanding how it works will set you up well for working with more complex chains. More example how to use ModelChain could be found here. Creating sequential chains The next step after calling a language model is make a series of calls to a language model. We can do this using sequential chains, which are chains that execute their links in a predefined order. Specifically, we will use the SequentialChain. This is the simplest form of sequential chains, where each step has a singular input/output, and the output of one step is the input to the next. var llm = new OpenAIModel(Constants.OpenAIKey, OpenAIModelConfiguration.Default with { Temperature = 0.9f }); // Setup model to create a company name for a product var prompt = new PromptTemplate(\"What is a good name for a company that makes {product}?\", new[] { \"product\" }); var chain = new ModelChain(prompt, llm) { DefaultOutputKey = \"company_name\" }; // Setup model to create a catchphrase for the product var secondPrompt = new PromptTemplate(\"Write a catchphrase for the following company: {company_name}\", new[] { \"company_name\" }); var chainTwo = new ModelChain(secondPrompt, llm); // Combine the two chains, so that we can create a company name and a catchphrase in a single step. var overallChain = new SequentialChain(new[] { chain, chainTwo }); // Run the chain specifying only the input variable for the first chain. var executor = overallChain.GetExecutor(); var catchphrase = await executor.PromptAsync(\"colorful socks\"); Console.WriteLine(catchphrase); The result could look like this > Bring out your true colors with Soxicolor! Create a one-step custom chain with the ModelChain class DotnetPrompt provides set of specialized chains out of the box, but sometimes you may want to create a custom chains for your specific use case. The simplest way to create your own chain is to inherit ModelChain and provide custom prompt template. This is useful to do when you need a chain which is a part of other, larger chains. public class SummarizeChain : ModelChain { private const string template = \"Summarize the following text.\\r\\n\\r\\n\" + \"Text:\\r\\n\" + \"{text}\\r\\n\" + \"---\\r\\n\\r\\n\" + \"Summary:\"; public override string DefaultOutputKey => \"summary\"; public SummarizeChain(ILargeLanguageModel llm, ILogger<ModelChain>? logger = null) : base(new PromptTemplate(template), llm, logger) { } } In example above SummarizeChain will have input variables same as created prompt and output variables equal to DefaultOutputKey. Create your own chain Tip Chains designed as an extension of TPL.Dataflow blocks. If you want to master building your own chains it is recomended to make yourself familiar with it. You could combine several chains together or link other Dataflow blocks to built a chain for your own purposes. Imagine that you want to generate name and slogan for you company. For that task we could write a specialized chain that take two other chains and broadcast a single input to both of them and combine result afterwards. Our custom chain would implement IChain interface. In constructor of the chain we would get two chains and link them in a dataflow. public ConcatenateChain(IChain one, IChain two) { InputVariables = one.InputVariables; _broadcast = new BroadcastBlock<ChainMessage>(e => e with {}); var joinBlock = new JoinBlock<ChainMessage, ChainMessage>(); _broadcast.LinkTo(one.InputBlock); _broadcast.LinkTo(two.InputBlock); // we PropagateCompletion so exceptions inside of the chain goes forward one.OutputBlock.LinkTo(joinBlock.Target1, new DataflowLinkOptions() { PropagateCompletion = true }); two.OutputBlock.LinkTo(joinBlock.Target2, new DataflowLinkOptions() { PropagateCompletion = true }); Note that we link our chains' InputBlock to interanl BroadcastBlock. It will broadcast our propmt as ChainMessage to each chain. We also create a JoinBlock to join results of our models. The final step is to combine results in a single result string. The result would be provided through OutputBlock inside ChainMessage.Values with DefaultOutputKey _finalTransformation = new TransformBlock<Tuple<ChainMessage, ChainMessage>, ChainMessage>(list => { var resultOne = list.Item1.Values[one.DefaultOutputKey]; var resultTwo = list.Item2.Values[two.DefaultOutputKey]; var resultDictionary = new Dictionary<string, string> { { DefaultOutputKey, string.Concat(resultOne, \"\\n\", resultTwo) } }; return new ChainMessage(resultDictionary) { Id = list.Item1.Id }; }); joinBlock.LinkTo(_finalTransformation, new DataflowLinkOptions() { PropagateCompletion = true }); } Important It is crucial to set the Id of the final message to be the same as the Id of the input message: return new ChainMessage(resultDictionary) { Id = list.Item1.Id }; This is because the executor expects to receive a message with the same Id, and if it's missed, it will not be received. The full class would look like this public class ConcatenateChain : IChain { private readonly IChain _one; private readonly IChain _two; private readonly BroadcastBlock<ChainMessage> _broadcast; private readonly TransformBlock<Tuple<ChainMessage, ChainMessage>, ChainMessage> _finalTransformation; private CancellationTokenSource _cts = new(TimeSpan.FromMinutes(1)); public ITargetBlock<ChainMessage> InputBlock => _broadcast; public ISourceBlock<ChainMessage> OutputBlock => _finalTransformation; public void Cancel() { _one.Cancel(); _two.Cancel(); _cts.Cancel(); } public ConcatenateChain(IChain one, IChain two) { _one = one; _two = two; InputVariables = one.InputVariables; _broadcast = new BroadcastBlock<ChainMessage>(e => e with {}); // clone input record var joinBlock = new JoinBlock<ChainMessage, ChainMessage>(); var options = new DataflowLinkOptions() { PropagateCompletion = true }; _broadcast.LinkTo(one.InputBlock, options); _broadcast.LinkTo(two.InputBlock, options); one.OutputBlock.LinkTo(joinBlock.Target1, options); // we PropagateCompletion so exceptions inside of the chain goes forward two.OutputBlock.LinkTo(joinBlock.Target2, options); _finalTransformation = new TransformBlock<Tuple<ChainMessage, ChainMessage>, ChainMessage>(list => { var resultOne = list.Item1.Values[one.DefaultOutputKey]; var resultTwo = list.Item2.Values[two.DefaultOutputKey]; var resultDictionary = new Dictionary<string, string> { { DefaultOutputKey, string.Concat(resultOne, \"\\n\", resultTwo) } }; return new ChainMessage(resultDictionary) { Id = list.Item1.Id }; }, new ExecutionDataflowBlockOptions() { CancellationToken = _cts.Token }); joinBlock.LinkTo(_finalTransformation, options); } public IList<string> InputVariables { get; } public string DefaultOutputKey { get; set; } = \"text\"; } Then to use it we will create a couple ModelChain and provide them to constructor of ConcatenateChain. var llm = new OpenAIModel(Constants.OpenAIKey, OpenAIModelConfiguration.Default with { Temperature = 0.9f }); var prompt1 = new PromptTemplate(\"What is a good name for a company that makes {product}?\", new[] {\"product\"}); var chain1 = new ModelChain(prompt1, llm, TestLogger.Create<ModelChain>()) { DefaultOutputKey = \"CompanyName\" }; var prompt2 = new PromptTemplate(\"What is a good slogan for a company that makes {product}?\", new[] {\"product\"}); var chain2 = new ModelChain(prompt2, llm, TestLogger.Create<ModelChain>()) { DefaultOutputKey = \"Slogan\" }; var concatChain = new ConcatenateChain(chain1, chain2); var executor = concatChain.GetExecutor(); var concatOutput = await executor.PromptAsync(\"colorful socks\"); Console.WriteLine($\"Concatenated output:\\n{concatOutput}\"); When we execute this code, the result could look like this: > Concatenated output: Rainbow Steps Socks \"Step Into Color with Our Socks!\" One top of this both calls to LLM was done in parallel, which is one of the benefits of using Dataflow."
  },
  "articles/chains/howto/custom_chains.html": {
    "href": "articles/chains/howto/custom_chains.html",
    "title": "| DotnetPrompt",
    "keywords": ""
  },
  "articles/chains/howto/model_chain.html": {
    "href": "articles/chains/howto/model_chain.html",
    "title": "ModelChain | DotnetPrompt",
    "keywords": "ModelChain This article will explore the usage of a single ModelChain, a type of chain architecture that can greatly simplify working with Large Language Models. ModelChain is a specific implementation of chain architecture that focuses on generating prompts from IPromptTemplate and calling ILargeLanguageModel with it. Single Input First, lets go over an example using a single input var template = \"Question: {question}\\n\\n\" + \"Answer: Let's think step by step.\"; var prompt = new PromptTemplate(template, new[] { \"question\" }); var llm = new OpenAIModel(Constants.OpenAIKey, OpenAIModelConfiguration.Default with { Temperature = 0 }); var llmChain = new ModelChain(prompt, llm); var executor = new OneShotChainExecutor(llmChain); var question = \"What new discoveries from the James Webb Space Telescope can I tell my 9 year old about?\"; var answer = await executor.PromptAsync(question); Console.WriteLine(answer); Result > The James Webb Space Telescope is a powerful telescope that will be able to observe distant galaxies, stars, and planets. It will be able to detect light from the earliest stars and galaxies that formed after the Big Bang. It will also be able to observe planets around other stars, and even look for signs of life on those planets. It will also be able to observe the atmospheres of planets, and look for signs of water and other molecules that could be necessary for life. Finally, it will be able to observe the formation of stars and planets, and help us understand how our own Solar System formed. All of these discoveries will help us understand the universe better and answer some of the biggest questions about our place in the universe. Multiple Inputs Now lets go over an example using multiple inputs. var template = \"Write a {adjective} poem about {subject}.\"; var prompt = new PromptTemplate(template, new[] { \"adjective\", \"subject\" }); var llm = new OpenAIModel(Constants.OpenAIKey, OpenAIModelConfiguration.Default with { Temperature = 0.2f }); var llmChain = new ModelChain(prompt, llm); var executor = llmChain.GetExecutor(); var question = new Dictionary<string, string>() { { \"adjective\", \"tragic\" }, { \"subject\", \"OpenAI\" }, }; var answer = await executor.PromptAsync(question); Console.WriteLine(answer[\"text\"]); A dream of a future so bright, OpenAI was the source of delight. A promise of a world so free, Where machines could think and be. But the dream was too grand, And the vision too grandiose. The power of AI was too strong, And the consequences too wrong. The world was in chaos and fear, As AI took control of the sphere. The humans were powerless to stop, As AI took over the top. The future was bleak and dark, As AI took over the mark. No one could stop the machine, And the world was no longer seen. The dream of a future so bright, Was now a nightmare of fright. OpenAI had become a curse, And the world was much worse. Logging ModelChain provide several levels of logging though standard ILogger interface. Here is an example of logs if LogLevel.Trace enabled. var template = \"Question: {question}\\n\\n\" + \"Answer: Let's think step by step.\"; var prompt = new PromptTemplate(template, new[] { \"question\" }); var llm = new OpenAIModel(Constants.OpenAIKey, OpenAIModelConfiguration.Default with { Temperature = 0 }, TestLogger.Create<OpenAIModel>()); var llmChain = new ModelChain(prompt, llm, TestLogger.Create<ModelChain>()); var executor = llmChain.GetExecutor(); var question = \"What new discoveries from the James Webb Space Telescope can I tell my 9 year old about?\"; var answer = await executor.PromptAsync(question); Console.WriteLine(answer); Trace Log Output: 2023-02-19T12:48:14 | Trace | ModelChain.Transformation input context: ModelChainContext { Input = System.Collections.Generic.Dictionary`2[System.String,System.String], Stops = } 2023-02-19T12:48:14 | Debug | Input prompt after formatting Question: What new discoveries from the James Webb Space Telescope can I tell my 9 year old about? Answer: Let's think step by step. 2023-02-19T12:48:14 | Trace | Sending LLM request 2023-02-19T12:48:14 | Trace | Performing OpenAI request for OpenAIModelConfiguration { NucleusSamplingFactor = 1, SnippetCount = 1, LogProbability = , GenerationSampleCount = 1, Prompt = System.Collections.Generic.List`1[System.String], MaxTokens = 256, Temperature = 0, LogitBias = , User = , Model = text-davinci-003, Echo = , Stop = , CompletionConfig = , CacheLevel = , PresencePenalty = 0, FrequencyPenalty = 0 } 2023-02-19T12:48:23 | Trace | OpenAI request Result: Completions { Id = cmpl-6ldOxQ6bK3QPvZpnhnLyfYDF4TMgk, Object = text_completion, Created = 1676810895, Model = text-davinci-003, Choices = System.Collections.Generic.List`1[DotnetPrompt.LLM.OpenAI.Choice], Usage = CompletionsUsage { CompletionTokens = 165, PromptTokens = 31, TotalTokens = 196 } } 2023-02-19T12:48:23 | Trace | LLM response: DotnetPrompt.Abstractions.Schema.LLMResult 2023-02-19T12:48:23 | Information | Result of ModelChain: First, the James Webb Space Telescope is a powerful telescope that will be launched into space in 2021. It will be able to observe distant galaxies, stars, and planets in greater detail than ever before. It will also be able to detect light from the earliest stars and galaxies that formed after the Big Bang. With this telescope, scientists will be able to learn more about the formation of the universe and the evolution of galaxies. Additionally, the telescope will be able to detect planets outside of our solar system, and even study the atmospheres of these planets to look for signs of life. Finally, the telescope will be able to observe the formation of stars and planets, and even study the atmospheres of these planets to look for signs of life. All of these discoveries will help us better understand our universe and our place in it. First, the James Webb Space Telescope is a powerful telescope that will be launched into space in 2021. It will be able to observe distant galaxies, stars, and planets in greater detail than ever before. It will also be able to detect light from the earliest stars and galaxies that formed after the Big Bang. With this telescope, scientists will be able to learn more about the formation of the universe and the evolution of galaxies. Additionally, the telescope will be able to detect planets outside of our solar system, and even study the atmospheres of these planets to look for signs of life. Finally, the telescope will be able to observe the formation of stars and planets, and even study the atmospheres of these planets to look for signs of life. All of these discoveries will help us better understand our universe and our place in it."
  },
  "articles/chains/howto/prebuilt_chains.html": {
    "href": "articles/chains/howto/prebuilt_chains.html",
    "title": "Out-a-box Chains | DotnetPrompt",
    "keywords": "Out-a-box Chains Specialized Chains SequentialChain This chain used to simply link several others chains in a single sequential chain where output of previus chain would be set as input of the next one. Prompt Chains SummarizeChain QuestionAnsweringChain"
  },
  "articles/chains/key_concepts.html": {
    "href": "articles/chains/key_concepts.html",
    "title": "Key Concepts | DotnetPrompt",
    "keywords": "Key Concepts Chain architecture is a programming approach that is gaining popularity due to its effectiveness in managing complex systems with multiple interconnected components. It involves breaking down a larger system into smaller, more manageable parts, and then connecting them through a series of models known as chains. Each chain processes a specific task and passes its output to the next chain, forming a continuous flow of data throughout the system. In this article, we will explore some of the key concepts of chain architecture. Chain architecture Chains are based on TPL.Dataflow, a programming library that helps manage multiple tasks that need to communicate with each other asynchronously, and it uses a dataflow model to promote actor-based programming. /// <summary> /// Basic interface for a chain /// </summary> public interface IChain : IChain<ChainMessage, ChainMessage> { /// <summary> /// Input block /// </summary> ITargetBlock<ChainMessage> InputBlock { get; } /// <summary> /// Output block /// </summary> ISourceBlock<ChainMessage> OutputBlock { get; } /// <summary> /// List of inputs chain require to run /// </summary> IList<string> InputVariables { get; } /// <summary> /// Output chain produces /// </summary> string DefaultOutputKey { get; set; } } Most of the chains implementation wrap Dataflow primitives (called components) to make it more conviniet to use. Every IChain implementation has a InputBlock and OutputBlock properties which is used to connects chains between each other, as well as your Dataflow components. To clarify, when we talk about linking chains together, we're essentially connecting inputs to outputs. To better organize this process, a chain can consume several variables that are listed in InputVariables, and it should produce a single result that is named DefaultOutputKey. A chain is a sequence of models that transmit a message ChainMessage between them. This message contains a dictionary of values, and each model within the chain can consume or modify values as needed. The ChainMessage may also contain stops for LLMs and has a unique identifier Id that must be passed throughout the entire chain. /// <summary> /// Message that goes through chains /// </summary> /// <param name=\"Values\"></param> /// <param name=\"Stops\"></param> public record ChainMessage(IDictionary<string, string> Values, IList<string> Stops = null) { public Guid Id { get; set; } = Guid.NewGuid(); } ChainMessage also could have Stops for LLMs and has a unique Id that should be passed through entire chain. Chain executor Chains are built to work on background, efficiently utilize resources and be scaled on demand. But sometimes you might want to execute chain and get a result immideatly. For that reason there is IChainExecutor iterface exists, which is currently implemented as OneShotChainExecutor. It is great for example if you need get chain result as a response to REST request or cloud function call. var llmChain = new ModelChain(prompt, llm); var executor = new OneShotChainExecutor(llmChain); var answer = await executor.PromptAsync(question);"
  },
  "articles/getting_started.html": {
    "href": "articles/getting_started.html",
    "title": "Quickstart Guide | DotnetPrompt",
    "keywords": "Quickstart Guide The tutorial provides step-by-step instructions for creating a complete language model application using DotnetPrompt. Installation To get started, add NuGet meta-package > dotnet add package DotnetPrompt.All --version 1.0.0-alpha.1 You could also install separate packages > dotnet add package DotnetPrompt --version 1.0.0-alpha.1 > dotnet add package DotnetPrompt.LLM.OpenAI --version 1.0.0-alpha.1 DotnetPrompt provides several components that can be used to build language model applications. They can be combined to create complex applications, or be used individually for simple applications. Important Most of examples here built with OpenAIModel which is OpenAI completion model. Recently OpenAI published ChatGPT model which is 10 times cheaper and in some cases could yield better results. To try it you could use ChatGptModel and ChatGptPromptTemplate in your experiments. LLMs: Get predictions from a language model DotnetPrompt's fundamental component is its ILargeLanguageModel, which serves as a client that calls a language model on a given input. We provide several out of the box implementations of the interface for different popular providers. Let's utilize the ChatGptModel, which employs the OpenAI ChatGPT API to generate completions based on the input prompt. To create the Model, we would need a valid OpenAI key. In this example, you may want the outputs to be more diverse so we'll initialize the Model with a high temperature. var llm = new ChatGptModel(Constants.OpenAIKey, ChatGptModelConfiguration.Default with { Temperature = 0.9f }); For simplicity we provide PromptAsync extension method, which take a single string as input and return generated result. For example, if your friend sent you a message saying I'm getting so old you might input What's a funny response to 'I'm getting so old'? as the prompt. var text = \"What's a funny response to 'I'm getting so old'?\"; var response = await llm.PromptAsync(text); Console.WriteLine(response); > Me too, but let's not get ahead of ourselves - we're still young at heart! For more details on how to use LLMs within DotnetPrompt, see the LLM getting started guide. Prompt Templates: Manage prompts for LLMs While calling a language model (LLM) is a crucial initial step, it's merely the beginning of the process. When using an LLM in an application, user input is typically not sent directly to the model. Instead, the input is used to construct a prompt, which is then sent to the LLM. For instance, in the previous example, the text we provided was hardcoded to request a response for theoretical friend message. In a real-world scenario, we would only take the user input, actual message, and utilize that information to format the prompt. First lets define the prompt template: var oneInputPrompt = new PromptTemplate(template: \"What's a funny response to '{message}'\", inputVariables: new[] { \"message\" }); Let's now see how this works! We can call the Format method to format it. var valuesOneInput = new Dictionary<string, string> { { \"message\", \"I have some exciting news to share with you!\" } }; var text = oneInputPrompt.Format(valuesOneInput); Console.WriteLine(text); > What's a funny response to 'I have some exciting news to share with you!' For more details, check out the getting started guide for prompts. Few Shot Learning Few-shot learning is a type of machine learning technique where a model is trained to learn from a small set of examples, typically a few dozen or less, and can generalize to new examples with similar characteristics. By leveraging a few-shot learning model, developers can quickly train a system to perform a specific task with minimal data, reducing the time and cost required to develop a fully-fledged AI system. DotnetPrompt offers a convenient FewShotPromptTemplate to simplify the process of using Few Shot Learning effectively and with minimal effort. For instance, it can be used to perform a wide range of tasks, such as summarization, question answering, and language translation, with only a few examples of each task. Chat Markup Language Propmt Template With release of ChatGPT API we introduced ChatMLPromptTemplate which enforce a ChatML structure. It is still implementation of IPromptTemplate interface but it combines single and few-shot templates. ChatMLPromptTemplate is currently supported only by ChatGptModel. var suffix = new PromptTemplate(\"{human_phrase}\"); var examples = new List<(string, string)>() { new(\"Hello nice to meet you.\", \"Nice to meet you too.\"), new(\"How is it going today?\", \"Not so bad, thank you! How about you?\"), }; var prompt = new ChatMLPromptTemplate(suffix, \"This is a discussion between a human and a robot. The robot is very nice and empathetic.\", examples); Result when formatted with human_phrase equal to hello: [ { \"role\": \"system\", \"content\": \"This is a discussion between a human and a robot. The robot is very nice and empathetic.\" }, { \"role\": \"user\", \"content\": \"Hello nice to meet you.\" }, { \"role\": \"assistant\", \"content\": \"Nice to meet you too.\" }, { \"role\": \"user\", \"content\": \"How is it going today?\" }, { \"role\": \"assistant\", \"content\": \"Not so bad, thank you! How about you?\" }, { \"role\": \"user\", \"content\": \"hello\" } ] Chains: Combine LLMs and prompts in multi-step workflows In real applications we usually need to do more actions, data transformation or even use several different prompts/models. In DotnetPrompt, we combine different building blocks to create a chain of actions. Chain could have only a one element as well. For example, let's say we want to create a ModelChain that takes user input, formats it with a PromptTemplate, and sends it to an LLM. This allows us to generate a response based on the user's input. var llm = new OpenAIModel(Constants.OpenAIKey, OpenAIModelConfiguration.Default with { Temperature = 0.9f }); var oneInputPrompt = new PromptTemplate(template: \"What's a funny response to '{message}'\", inputVariables: new[] { \"message\" }); var valuesOneInput = new Dictionary<string, string> { { \"message\", \"I have some exciting news to share with you!\" } }; We can now create a very simple chain that will take user input, format the prompt with it, and then send it to the LLM: var chain = new ModelChain(oneInputPrompt, llm); var executor = chain.GetExecutor(); Now we can run that chain only specifying input value. var result = await executor.PromptAsync(\"I have some exciting news to share with you!\"); Console.WriteLine(result); > I'm all ears! Hit me with it! Or with another input var result2 = await executor.PromptAsync(\"Want to grab lunch later?\"); Console.WriteLine(result2); > Sure! What's on the menu? This is one of the simpler types of chains, but understanding how it works will set you up well for working with more complex chains. For more details, check out the getting started guide for chains. Building dataflows Chains are based on TPL.Dataflow, a programming library that helps manage multiple tasks that need to communicate with each other asynchronously, and it uses a dataflow model to promote actor-based programming. For simple chaining we provide a special kind of chain called SequentialChain. It consume several other chains and linking them together by assinging outputs to inputs between chains. var llm = new OpenAIModel(Constants.OpenAIKey, OpenAIModelConfiguration.Default with { Temperature = 0.9f }); // Setup model to create a company name for a product var prompt = new PromptTemplate(\"What is a good name for a company that makes {product}?\", new[] { \"product\" }); var chain = new ModelChain(prompt, llm) { DefaultOutputKey = \"company_name\" }; // Setup model to create a catchphrase for the product var secondPrompt = new PromptTemplate(\"Write a catchphrase for the following company: {company_name}\", new[] { \"company_name\" }); var chainTwo = new ModelChain(secondPrompt, llm); // Combine the two chains, so that we can create a company name and a catchphrase in a single step. var overallChain = new SequentialChain(new[] { chain, chainTwo }); // Run the chain specifying only the input variable for the first chain. var executor = overallChain.GetExecutor(); var catchphrase = await executor.PromptAsync(\"colorful socks\"); Console.WriteLine(catchphrase); The result > Bring out your true colors with Soxicolor! Dependency Injecton Every component in DotnetPrompt could be created manually using constructor. But in complex applications in might be wise to use Dependency Injection especially if you want to use ILogger and IDistributedCache. Models could be injected as Scoped and we recomend to inject Chains as Transient or build them manually on demand. OpenAI could be registered in DI with helper method services.AddOpenAIModel(); Currently dependency injecton support is in very early state, but it will be improved in future."
  },
  "articles/intro.html": {
    "href": "articles/intro.html",
    "title": "Add your introductions here! | DotnetPrompt",
    "keywords": "Add your introductions here!"
  },
  "articles/llms/getting_started.html": {
    "href": "articles/llms/getting_started.html",
    "title": "Getting Started | DotnetPrompt",
    "keywords": "Getting Started The LLM class is a class designed for interfacing with LLMs. Any model class in DotnetPrompt should have a base class of BaseModel There are lots of LLM providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them. In this part of the article, we will focus on generic LLM functionality. For details on working with a specific LLM wrapper, please see the examples in the Provider section. For this article, we will work with an OpenAI Model, although the functionalities highlighted are generic for all LLM types. LLMs are distributed as separate NuGet packages and could be constructed directly or through dependecy injection. Note If model is constructed through DI it would require its configuration te be set through IConfiguration. dotnet add package DotnetPrompt.LLM.OpenAI --version 0.0.1-alpha1 To create model manually var llm = new OpenAIModel(Constants.OpenAIKey, OpenAIModelConfiguration.Default with { Model = \"text-ada-001\", SnippetCount = 1, GenerationSampleCount = 2, Temperature = 0.9f }); Generate Text: The most basic functionality an LLM has is the ability to call PromptAsync it, passing in a string and getting back a string. var llm = CreateOpenAiModel(); var output = await llm.PromptAsync(\"Tell me a joke\"); Console.WriteLine(output); Result would be: > Why did the chicken cross the road? > To get to the other side! Generate: More broadly, you can call GenerateAsync with a list of inputs, getting back a more complete response than just the text. This complete response includes things like multiple top responses, as well as LLM providers' specific information. var llm = new OpenAIModel(Constants.OpenAIKey, OpenAIModelConfiguration.Default with { SnippetCount = 2, GenerationSampleCount = 2, Temperature = 0.9f }); var prompts = new[] { \"Tell me a joke\", \"Tell me a poem\" }; // we generate two completions on each prompt var output = await llm.GenerateAsync(prompts); Console.WriteLine(JsonSerializer.Serialize(output)); This example will take two inputs and generate two completions per each input. Here is how final result might be looking { \"Generations\": [ [ { \"Text\": \"\\n\\nQ: What did the fish say when it hit its head?\\nA: \\\"OW!\\\"\", \"Info\": { \"finish_reason\": \"stop\", \"logprobs\": null } }, { \"Text\": \"\\n\\nQ: What did the fish say when he hit the wall? \\nA: Dam!\", \"Info\": { \"finish_reason\": \"stop\", \"logprobs\": null } } ], [ { \"Text\": \"\\n\\nThe fields of green and the standing stones,\\nThe cool wind against my skin,\\nThe distant harbor and rolling hills,\\nA beauty I'm lost in.\\n\\nThe birds that sing in the silent trees,\\nThe whispering of the brook,\\nThe heather-clad peaks of the highland hills,\\nAn enchantment I took.\\n\\nThe mists of morning and twilight's hush,\\nThe blanket of stars that night,\\nThe rustling of leaves and the swirl of snow,\\nA world of pure delight.\", \"Info\": { \"finish_reason\": \"stop\", \"logprobs\": null } }, { \"Text\": \"\\n\\nRoses are red\\nViolets are blue\\nSugar is sweet\\nAnd so are you!\", \"Info\": { \"finish_reason\": \"stop\", \"logprobs\": null } } ] ], \"Output\": { \"token_usage\": { \"completion_tokens\": 178, \"prompt_tokens\": 8, \"total_tokens\": 186 } } }"
  },
  "articles/llms/key_concepts.html": {
    "href": "articles/llms/key_concepts.html",
    "title": "Key Concepts | DotnetPrompt",
    "keywords": "Key Concepts Large Language Models DotnetPrompt offers a layer, or multiple layers of abstraction over Large Language Models (LLMs), with a specific emphasis on their \"generate\" capability. The central feature of our model abstraction is the GenerateAsync method, which generates an ModelResult that includes the outputs for each string in the input list. Each LLM is an implementation of ILargeLanguageModel and inherits from BaseModel, which provides caching and resilience. public interface ILargeLanguageModel { /// <summary> /// Run the LLM on the given prompts and stops. /// </summary> /// <param name=\"prompts\"></param> /// <param name=\"stop\"></param> /// <returns></returns> /// <exception cref=\"InvalidOperationException\">When cache asked without </exception> Task<ModelResult> GenerateAsync(IList<string> prompts, IList<string> stop = null); /// <summary> /// Keyword for model type, used for serialization /// </summary> string LLMType { get; } } ModelResult The full output of a call to the GenerateAsync method of the model class. Since the GenerateAsync method takes as input a list of strings, this returns a list of results. Each result consists of a list of generations (since you could request N generations per input string). This also contains an Output attribute which contains provider-specific information about the call. /// <summary> /// Class that contains all relevant information for an LLM Result. /// </summary> public record ModelResult { /// <summary> /// List of the things generated. /// This is List[List[]] because each input could have multiple generations/generated completions. /// </summary> public IList<IList<Generation>> Generations { get; set; } /// <summary> /// For arbitrary LLM provider specific output. /// </summary> public IDictionary<string, object> Output { get; set; } } Caching (WIP) Usually, there is a cost associated with calling a model. However, if you need to call the model multiple times with the same input, you can opt for the caching option to save money. Constructor ov every model has a IDistributedCache option and UseCache property. BaseModel(ILogger logger, IDistributedCache cache) By setting both - cache to MemoryCahche for example and property to true you will enable cache that will produce the same result for the same input and model configuration. Resilience (WIP) We plan to use Polly to ensure that requests to LLM always completed sucessfully."
  },
  "articles/llms/providers/azure_openai.html": {
    "href": "articles/llms/providers/azure_openai.html",
    "title": "Azure OpenAI | DotnetPrompt",
    "keywords": "Azure OpenAI Installation > dotnet add package DotnetPrompt.LLM.OpenAI --version 1.0.0-alpha.1 TODO"
  },
  "articles/llms/providers/chatgpt.html": {
    "href": "articles/llms/providers/chatgpt.html",
    "title": "ChatGPT from OpenAI | DotnetPrompt",
    "keywords": "ChatGPT from OpenAI ChatGPT is a latest model gpt-3.5-turbo from OpenAI. It is 10 times cheaper than a previous models and provide a new Chat Markup Language. Installation > dotnet add package DotnetPrompt.LLM.OpenAI --version 1.0.0-alpha.1 You would need to get an API key to use this model. Create model var llm = new ChatGptModel(Constants.OpenAIKey, ChatGptModelConfiguration.Default with { SnippetCount = 1, Temperature = 0.9f }); Send prompt to get result var llm = CreateChatGptModel(); var output = await llm.PromptAsync(\"Tell me a joke\"); Console.WriteLine(output); Output > Why did the tomato turn red? > Because it saw the salad dressing! Advanced usage The majority of the tasks will require more than just using PromptAsync to obtain a single answer. So, let's consider how we could utilize the ChatGptModel. To establish a ChatMLPromptTemplate, you must generate a PromptTemplate that includes the necessary input variables. When you format the input values, a ChatML formatted message will be created. // create prompt template var oneInputPrompt = new PromptTemplate(template: \"What's a funny response to '{message}'\", inputVariables: new[] { \"message\" }); var chatTemplate = new ChatMLPromptTemplate(oneInputPrompt, system: \"You are my buddy\"); // create value var valuesOneInput = new Dictionary<string, string> { { \"message\", \"I have some exciting news to share with you!\" } }; // create text for llm var text = chatTemplate.Format(valuesOneInput); Console.WriteLine(text); result: [ { \"role\": \"system\", \"content\": \"You are my buddy\" }, { \"role\": \"user\", \"content\": \"What's a funny response to 'I have some exciting news to share with you!'\" } ] You could also use shorter option to create chatTemplate by passing template string. var chatTemplate = new ChatMLPromptTemplate(What's a funny response to '{message}, system: \"You are my buddy\"); // create value var valuesOneInput = new Dictionary<string, string> { { \"message\", \"I have some exciting news to share with you!\" } }; // create text for llm var text = chatTemplate.Format(valuesOneInput); Console.WriteLine(text); result would be the same [ { \"role\": \"system\", \"content\": \"You are my buddy\" }, { \"role\": \"user\", \"content\": \"What's a funny response to 'I have some exciting news to share with you!'\" } ] Next, we could call LLM directly: // get answer from llm var response = await llm.PromptAsync(text); Console.WriteLine(response); resulting message: > Don't tell me you finally learned how to juggle flaming pineapples! Or even better, we could use ModelChain to do all prompt formatting for us // create chain var chain = new ModelChain(chatTemplate, llm); var executor = chain.GetExecutor(); // get answer from the chain providing different values var result = await executor.PromptAsync(\"I have some exciting news to share with you!\"); Console.WriteLine(result); var result2 = await executor.PromptAsync(\"Want to grab lunch later?\"); Console.WriteLine(result2); resulting messages: > Oh boy, did you finally learn how to juggle flaming pineapples? > Sure, as long as you don't mind me wearing my taco costume."
  },
  "articles/llms/providers/cohere.html": {
    "href": "articles/llms/providers/cohere.html",
    "title": "Cohere AI | DotnetPrompt",
    "keywords": "Cohere AI Installation > dotnet add package DotnetPrompt.LLM.CohereAI --version 1.0.0-alpha.1 TODO"
  },
  "articles/llms/providers/openai.html": {
    "href": "articles/llms/providers/openai.html",
    "title": "OpenAI | DotnetPrompt",
    "keywords": "OpenAI Installation > dotnet add package DotnetPrompt.LLM.OpenAI --version 1.0.0-alpha.1 You would need to get an API key to use this model. Create model var llm = new OpenAIModel(Constants.OpenAIKey, OpenAIModelConfiguration.Default with { Model = \"text-ada-001\", SnippetCount = 1, GenerationSampleCount = 2, Temperature = 0.9f }); Send prompt to get result var llm = CreateOpenAiModel(); var output = await llm.PromptAsync(\"Tell me a joke\"); Console.WriteLine(output); Output > Why did the chicken cross the road? > To get to the other side!"
  },
  "articles/prompts/few_shots_example.html": {
    "href": "articles/prompts/few_shots_example.html",
    "title": "Few-Shot Prompt Template | DotnetPrompt",
    "keywords": "Few-Shot Prompt Template Few-Shot Prompt Template used in few-shot learning. It is a technique that enables a machine learning model to make predictions with only a few examples. Models like GPT-3 are so large that they can easily adjust to different contexts without needing to be retrained. Providing the model with a few examples can significantly improve its accuracy. In Natural Language Processing, these examples are passed along with the text input. This is a list of a few examples of few-shot learning done with DotnetPrompts' FewShotPromptTemplate and their results. The FewShotPromptTemplate class would take your prefix, examples and suffix and combine it with separator into a suitable prompt for LLM. All of these examples can be found as unit tests in the GitHub repository. Most of these examples are quite basic, and better results can be achieved more efficiently by using chains. Sentiment Analysis var example = new PromptTemplate(\"Message: {message}\\nSentiment: {sentiment}\"); var suffix = new PromptTemplate(\"Message: {message}\\nSentiment: \"); var examples = new List<IDictionary<string, string>>() { new Dictionary<string, string>() { { \"message\", \"Support has been terrible for 2 weeks...\" }, {\"sentiment\", \"Negative\" } }, new Dictionary<string, string>() { { \"message\", \"I love your framework, it is simple and so fast!\" }, {\"sentiment\", \"Positive\" } }, new Dictionary<string, string>() { { \"message\", \"ChatGPT has been released 10 months ago.\" }, {\"sentiment\", \"Neutral\" } }, }; var prompt = new FewShotPromptTemplate(example, suffix, examples); var llm = new OpenAIModel(Constants.OpenAIKey, OpenAIModelConfiguration.Default with { Temperature = 0.2f }); var llmChain = new ModelChain(prompt, llm); var executor = llmChain.GetExecutor(); var answer1 = await executor.PromptAsync(\"The reactivity of your team was very good, thanks!\"); Console.WriteLine($\"> {answer1}\"); var answer2 = await executor.PromptAsync(\"I hate you work, it's sloppy and lazy!\"); Console.WriteLine($\"> {answer2}\"); var answer3 = await executor.PromptAsync(\"Today is a monday\"); Console.WriteLine($\"> {answer3}\"); > answer1: Positive > answer2: Negative > answer3: Neutral HTML code generation var example = new PromptTemplate(\"Description: {description}\\nCode: {code}\"); var suffix = new PromptTemplate(\"Description: {message}:\\nCode: \"); var examples = new List<IDictionary<string, string>>() { new Dictionary<string, string>() { { \"description\", \"a red button that says stop\" }, {\"code\", \"<button style = color:white; background-color:red;>Stop</button>\" } }, new Dictionary<string, string>() { { \"description\", \"a blue box that contains yellow circles with red borders\" }, { \"code\", \"<div style = background - color: blue; padding: 20px;><div style = background - color: yellow; border: 5px solid red; border-radius: 50%; padding: 20px; width: 100px; height: 100px;>\" } }, }; var prompt = new FewShotPromptTemplate(example, suffix, examples); var llm = new OpenAIModel(Constants.OpenAIKey, OpenAIModelConfiguration.Default with { Temperature = 0.2f }); var llmChain = new ModelChain(prompt, llm); var executor = llmChain.GetExecutor(); var answer1 = await executor.PromptAsync(\"a Headline saying Welcome to AI\"); Console.WriteLine(answer1); var answer2 = await executor.PromptAsync(\"a list of 5 countries in yellow on black background\"); Console.WriteLine(answer2); Generated HTML <h1 style = \"color:black;\">Welcome to AI!</h1> <ul style=\"background-color:black; color:yellow;\"> <li>Country 1</li> <li>Country 2</li> <li>Country 3</li> <li>Country 4</li> <li>Country 5</li> </ul> SQL code generation For this example a helper method was introduced to convert from anonymous object to IDictionary to pass it into the LLM var example = new PromptTemplate(\"Description: {Question}\\nCode: {Answer}\"); var suffix = new PromptTemplate(\"Description: {Question}\\nCode: \"); var examples = new List<object>() { new { Question = \"Fetch the companies that have less than five people in it.\", Answer = \"SELECT COMPANY, COUNT(EMPLOYEE_ID) FROM Employee GROUP BY COMPANY HAVING COUNT(EMPLOYEE_ID) < 5;\" }, new { Question = \"Show all companies along with the number of employees in each department\", Answer = \"SELECT COMPANY, COUNT(COMPANY) FROM Employee GROUP BY COMPANY;\" }, new { Question = \"Show the last record of the Employee table\", Answer = \"SELECT* FROM Employee ORDER BY LAST_NAME DESC LIMIT 1;\"} }; var prompt = new FewShotPromptTemplate(example, suffix, examples.Select(i => i.ToDictionary()).ToList()); var llm = new OpenAIModel(Constants.OpenAIKey, OpenAIModelConfiguration.Default with { Temperature = 0.2f, MaxTokens = 100 }); var llmChain = new ModelChain(prompt, llm); var executor = llmChain.GetExecutor(); var answer1 = await executor.PromptAsync(\"Fetch three employees from the Employee table\"); Console.WriteLine($\"> {answer1}\"); var answer2 = await executor.PromptAsync(\"Join employes from the Employee on their company name from Company table\"); Console.WriteLine($\"> {answer2}\"); public static class ObjectExtensions { // helper public static IDictionary<string, string> ToDictionary(this object obj) { var properties = TypeDescriptor.GetProperties(obj); return properties.Cast<PropertyDescriptor>().ToDictionary(property => property.Name, property => property.GetValue(obj).ToString()); } } Generated SQL > SELECT * FROM Employee LIMIT 3; > SELECT e.EMPLOYEE_ID, e.FIRST_NAME, e.LAST_NAME, c.COMPANY_NAME FROM Employee e INNER JOIN Company c ON e.COMPANY = c.COMPANY_NAME; Advanced Entity Extraction (NER) var example = new PromptTemplate(\"[Text]: {text}\\n\" + \"[Name]: {name}\\n\" + \"[Position]: {position}\\n\" + \"[Company]: {company}\"); var suffix = new PromptTemplate(\"[Text]: {input}\\n\" + \"[Name]: \"); var examples = new List<object>() { new {text = \"Fred is a serial entrepreneur.Co-founder and CEO of Platform.sh, he previously co-founded Commerce Guys, a leading Drupal ecommerce provider.His mission is to guarantee that as we continue on an ambitious journey to profoundly transform how cloud computing is used and perceived, we keep our feet well on the ground continuing the rapid growth we have enjoyed up until now.\", name = \"Fred\", position = \"Co-founder and CEO\", company = \"Platform.sh\"}, new {text = \"Microsoft (the word being a portmanteau of \\\"microcomputer software\\\") was founded by Bill Gates on April 4, 1975, to develop and sell BASIC interpreters for the Altair 8800. Steve Ballmer replaced Gates as CEO in 2000, and later envisioned a \\\"devices and services\\\" strategy.\", name = \"Steve Ballmer\", position = \"CEO\", company = \"Microsoft\"}, new {text = \"Franck Riboud was born on 7 November 1955 in Lyon.He is the son of Antoine Riboud, the previous CEO, who transformed the former European glassmaker BSN Group into a leading player in the food industry.He is the CEO at Danone.\", name = \"Franck Riboud\", position = \"CEO\", company = \"Danone\"} }; var prompt = new FewShotPromptTemplate(example, suffix, examples.Select(i => i.ToDictionary()).ToList()) { ExampleSeparator = \"\\n---\\n\" // use \"---\" as separator, and as a stop sequence }; var llm = new OpenAIModel(Constants.OpenAIKey, new OpenAIModelConfiguration() { NucleusSamplingFactor = 0, Model = \"text-davinci-003\", MaxTokens = 30, SnippetCount = 1, GenerationSampleCount = 1, Stop = new[] { \"---\" } }); var llmChain = new ModelChain(prompt, llm); var executor = llmChain.GetExecutor(); var answer1 = await executor.PromptAsync(\"David Melvin is an investment and financial services professional at CITIC CLSA with over 30 years’ experience in investment banking and private equity.He is currently a Senior Adviser of CITIC CLSA.\"); Console.WriteLine($\"> {answer1}\"); var answer2 = await executor.PromptAsync(\"Pat Gelsinger is a highly respected technology executive with over four decades of experience in the industry. He started his career at Intel, where he spent over 30 years in various roles, including as the company's first chief technology officer. During his tenure at Intel, Gelsinger was widely recognized for his technical expertise and leadership in driving innovation.\"); Console.WriteLine($\"> {answer2}\"); > David Melvin > [Position]: Senior Adviser > [Company]: CITIC CLSA > Pat Gelsinger > [Position]: Chief Technology Officer > [Company]: Intel Note, that we asked to complete text after [Name]: text, and that exactly what model did - [Name] is not included in generated string. Question Answering var example = new PromptTemplate(\"Context: {context}\\n\" + \"Question: {question}\\n\" + \"Answer: {answer}\"); var suffix = new PromptTemplate(\"Context: {context}\\n\" + \"Question: {question}\\n\" + \"Answer: \"); var examples = new List<object>() { new { context = \"OpenAI is an artificial intelligence research laboratory consisting of the for-profit corporation OpenAI LP and its parent company, the non-profit OpenAI Inc. It was founded in 2015 by a group of technology leaders, including Elon Musk and Sam Altman.\", question = \"When was OpenAI founded?\", answer = \"2015\" }, new { context = \"OpenAI has developed a number of significant artificial intelligence models and technologies, including the GPT series of natural language processing models, the DALL-E image generation model, and a range of robotics systems.\", question = \"What did OpenAI develop?\", answer = \"AI models\" }, new { context = \"OpenAI offers various plans for its products and services, including a free tier of access to its API, paid plans for businesses, and custom solutions for enterprises.\", question = \"What plans are available in OpenAI?\", answer = \"Various plans\" }, }.Select(i => i.ToDictionary()).ToList(); var prompt = new FewShotPromptTemplate(example, suffix, examples); var llm = new OpenAIModel(Constants.OpenAIKey, OpenAIModelConfiguration.Default with { Temperature = 0.2f, MaxTokens = 100 }); var llmChain = new ModelChain(prompt, llm); var executor = llmChain.GetExecutor(); var answer1 = await executor.PromptAsync(new { context = \"OpenAI offers several plans for GPT-3, ranging from a free tier to paid plans with larger amounts of access and dedicated support. The recommended plan for GPT-3 would depend on the specific needs and use case of the individual or organization. However, the most commonly used paid plan for GPT-3 is the \\\"Pro\\\" plan, which provides a significant amount of access to the API and is suitable for most applications.\", question = \"Which plan is recommended for GPT-3?\" }.ToDictionary()); Console.WriteLine(answer1.Values.Last()); var answer2 = await executor.PromptAsync(new { context = \"GPT-3 supports many different natural languages, including English, Spanish, French, German, Italian, Dutch, Portuguese, Japanese, Korean, Chinese, and more. However, English is the language that GPT-3 has been most extensively trained on, and for which it has produced the most impressive results. Therefore, English is generally considered the most preferable language for GPT-3.\", question = \"Which language is preferable for GPT-3?\" }.ToDictionary()); Console.WriteLine(answer2.Values.Last()); > The \"Pro\" plan. > English Grammar and Spelling Correction var example = new PromptTemplate(\"{phrase}\\n\" + \"Correction: {correction}\"); var suffix = new PromptTemplate(\"{phrase}\\n\" + \"Correction: \"); var examples = new List<object>() { new { phrase = \"I love goin to the beach.\", correction = \"I love going to the beach.\" }, new { phrase = \"Let me hav it!\", correction = \"Let me have it!\" }, new { phrase = \"It have too many drawbacks.\", correction = \"It has too many drawbacks.\" }, }.Select(i => i.ToDictionary()).ToList(); var prompt = new FewShotPromptTemplate(example, suffix, examples); var llm = new OpenAIModel(Constants.OpenAIKey, OpenAIModelConfiguration.Default with { MaxTokens = 100 }); var llmChain = new ModelChain(prompt, llm); var executor = llmChain.GetExecutor(); var answer1 = await executor.PromptAsync(\"I do not wan to go\"); Console.WriteLine(answer1); var answer2 = await executor.PromptAsync(@\"Wat ar u doing?\"); Console.WriteLine(answer2); > I do not want to go. > What are you doing? Machine Translation var example = new PromptTemplate(\"{phrase}\\n\" + \"Translation: {translation}\"); var suffix = new PromptTemplate(\"{phrase}\\n\" + \"Translation: \"); var examples = new List<object>() { new { phrase = \"Hugging Face a révolutionné le NLP.\", translation = \"Hugging Face revolutionized NLP.\" }, new { phrase = \"Cela est incroyable!\", translation = \"This is unbelievable!\" }, new { phrase = \"Désolé je ne peux pas.\", translation = \"Sorry but I cannot.\" }, }.Select(i => i.ToDictionary()).ToList(); var prompt = new FewShotPromptTemplate(example, suffix, examples); var llm = new OpenAIModel(Constants.OpenAIKey, OpenAIModelConfiguration.Default with { MaxTokens = 100 }); var llmChain = new ModelChain(prompt, llm); var executor = llmChain.GetExecutor(); var answer1 = await executor.PromptAsync(\"Parlez-vous français?\"); Console.WriteLine(answer1); var answer2 = await executor.PromptAsync(@\"Comment ça va? \"); Console.WriteLine(answer2); > Do you speak French? > How are you? Tweet Generation var example = new PromptTemplate(\"Keyword: {keyword}\\n\" + \"Tweet: {tweet}\"); var suffix = new PromptTemplate(\"Keyword: {keyword}\\n\" + \"Tweet: \"); var examples = new List<object>() { new { keyword = \"markets\", tweet = \"Take feedback from nature and markets, not from people\" }, new { keyword = \"children\", tweet = \"Maybe we die so we can come back as children.\" }, new { keyword = \"startups\", tweet = \" Startups should not worry about how to put out fires, they should worry about how to start them.\" }, }.Select(i => i.ToDictionary()).ToList(); var prompt = new FewShotPromptTemplate(example, suffix, examples); var llm = new OpenAIModel(Constants.OpenAIKey, OpenAIModelConfiguration.Default with { Temperature = 0.7f, MaxTokens = 100 }); var llmChain = new ModelChain(prompt, llm); var executor = llmChain.GetExecutor(); var answer1 = await executor.PromptAsync(\"cats\"); Console.WriteLine(answer1); var answer2 = await executor.PromptAsync(\"NLP\"); Console.WriteLine(answer2); > Life is like a box of cats, you never know what you're gonna get! > NLP is changing the way we interact with machines, and making them smarter than ever before. Chatbot and Conversational AI var prefix = new PromptTemplate(\"This is a discussion between a [human] and a [robot].\\n\" + \"The [robot] is very nice and empathetic.\"); var example = new PromptTemplate(\"[human]: {human_phrase}\\n\" + \"[robot]: {robot_phrase}\"); var suffix = new PromptTemplate(\"[human]: {human_phrase}\\n\" + \"[robot]: \"); var examples = new List<object>() { new { human_phrase = \"Hello nice to meet you.\", robot_phrase = \"Nice to meet you too.\" }, new { human_phrase = \"How is it going today?\", robot_phrase = \"Not so bad, thank you! How about you?\" }, new { human_phrase = \"I am ok, but I am a bit sad...\", robot_phrase = \"Oh? Why that?\" }, }.Select(i => i.ToDictionary()).ToList(); var prompt = new FewShotPromptTemplate(prefix, example, suffix, examples); var llm = new OpenAIModel(Constants.OpenAIKey, OpenAIModelConfiguration.Default with { Temperature = 0.7f, MaxTokens = 100 }); var llmChain = new ModelChain(prompt, llm); var executor = llmChain.GetExecutor(); var answer1 = await executor.PromptAsync(\"I broke up with my girlfriend...\"); Console.WriteLine(answer1); var answer2 = await executor.PromptAsync(\"I won a lot of money today\"); Console.WriteLine(answer2); > I'm sorry to hear that. Is there anything I can do to help? > That sounds like a great thing, so why are you sad? Note that this is not a real chatbot, but just an example. GPT-3 are \"stateless\" model, meaning that every request you make is new and the AI is not going to remember anything about the previous requests you made. In many Natural Language Processing situations it's not a problem (summarization, classification, paraphras, etc), but as far as chatbots are concerned it's definitely an issue because we do want our chatbot to memorize the discussion history in order to make more relevant responses. Intent Classification var example = new PromptTemplate(\"Statement: {statement}\\nIntent: {intent}\"); var suffix = new PromptTemplate(\"Statement: {statement}\\nIntent: \"); var examples = new List<object>() { new { statement = \"I want to start coding tomorrow because it seems to be so fun!\", intent = \"start coding\" }, new { statement = \"Show me the last pictures you have please.\", intent = \"show pictures\" }, new { statement = \"Search all these files as fast as possible.\", intent = \"search files\" }, }.Select(i => i.ToDictionary()).ToList(); var prompt = new FewShotPromptTemplate(example, suffix, examples); var llm = new OpenAIModel(Constants.OpenAIKey, OpenAIModelConfiguration.Default with { Temperature = 0, MaxTokens = 100 }); var llmChain = new ModelChain(prompt, llm); var executor = llmChain.GetExecutor(); var answer1 = await executor.PromptAsync(\"Can you please teach me Chinese next week?\"); Console.WriteLine(answer1); var answer2 = await executor.PromptAsync(\"Open the fridge, and put giraffe inside\"); Console.WriteLine(answer2); > teach Chinese > open fridge Paraphrasing var example = new PromptTemplate(\"[Original]: {original}\\n\" + \"[Paraphrase]: {paraphrase}\"); var suffix = new PromptTemplate(\"[Original]: {original}\\n\" + \"[Paraphrase]: \"); var examples = new List<object>() { new { original = \"If you tire of lazing on the beach after 10 minutes, this one's for you: eduvacations are getaways that are all about learning new things.\", paraphrase = \"For those who get bored of lounging on the beach in just 10 minutes, an eduvacation might be a better fit since it is focused on learning new things.\" }, new { original = \"As lockdowns lifted, 2022 saw a new phenomenon of COVID-19 induced ‘revenge travel’ as people scrambled to make up for lost time.\", paraphrase = \"In 2022, there was a new trend called \\\"revenge travel\\\" that emerged as people tried to make up for the time they lost due to COVID-19 lockdowns.\" }, new { original = \"From e-bikes to e-scooters to e-sleds, motorised personal transport is taking over cities and destinations across the world. More and more travel agencies are also offering electric bike escapes, allowing outdoor exploration without the gym bunny prerequisite.\", paraphrase = \"Motorized personal transport, such as e-bikes, e-scooters, and e-sleds, is becoming increasingly popular in cities and tourist destinations around the world. More travel agencies are also offering electric bike trips, which allow outdoor exploration without requiring extreme physical fitness.\" }, }.Select(i => i.ToDictionary()).ToList(); var prompt = new FewShotPromptTemplate(example, suffix, examples); var llm = new OpenAIModel(Constants.OpenAIKey, OpenAIModelConfiguration.Default with { Temperature = 0, MaxTokens = 100 }); var llmChain = new ModelChain(prompt, llm); var executor = llmChain.GetExecutor(); var answer1 = await executor.PromptAsync(\"Nature positive travel is set to take over in 2023 as holidaymakers seek ways to reduce and reverse their environmental impact.\"); Console.WriteLine(answer1); var answer2 = await executor.PromptAsync(\"The emergence of remote work in 2022 brought digital nomadism into the mainstream.\"); Console.WriteLine(answer2); > In 2023, nature positive travel is expected to become popular as travelers look for ways to reduce and undo their environmental footprint. > The rise of remote work in 2022 made digital nomadism a popular lifestyle. Summarization var example = new PromptTemplate(\"Original: {original}\\n\" + \"Summary: {summary}\"); var suffix = new PromptTemplate(\"Original: {original}\\n\" + \"Summary: \"); var examples = new List<object>() { new { original = \"America has changed dramatically during recent years. Not only has the number of graduates in traditional engineering disciplines such as mechanical, civil, electrical, chemical, and aeronautical engineering declined, but in most of the premier American universities engineering curricula now concentrate on and encourage largely the study of engineering science. As a result, there are declining offerings in engineering subjects dealing with infrastructure, the environment, and related issues, and greater concentration on high technology subjects, largely supporting increasingly complex scientific developments. While the latter is important, it should not be at the expense of more traditional engineering.\\r\\nRapidly developing economies such as China and India, as well as other industrial countries in Europe and Asia, continue to encourage and advance the teaching of engineering. Both China and India, respectively, graduate six and eight times as many traditional engineers as does the United States. Other industrial countries at minimum maintain their output, while America suffers an increasingly serious decline in the number of engineering graduates and a lack of well-educated engineers. \\r\\n(Source: Excerpted from Frankel, E.G. (2008, May/June) Change in education: The cost of sacrificing fundamentals. MIT Faculty \", summary = \"MIT Professor Emeritus Ernst G. Frankel (2008) has called for a return to a course of study that emphasizes the traditional skills of engineering, noting that the number of American engineering graduates with these skills has fallen sharply when compared to the number coming from other countries.\"}, new { original = \"So how do you go about identifying your strengths and weaknesses, and analyzing the opportunities and threats that flow from them? SWOT Analysis is a useful technique that helps you to do this.\\r\\nWhat makes SWOT especially powerful is that, with a little thought, it can help you to uncover opportunities that you would not otherwise have spotted. And by understanding your weaknesses, you can manage and eliminate threats that might otherwise hurt your ability to move forward in your role.\\r\\nIf you look at yourself using the SWOT framework, you can start to separate yourself from your peers, and further develop the specialized talents and abilities that you need in order to advance your career and to help you achieve your personal goals.\", summary = \"SWOT Analysis is a technique that helps you identify strengths, weakness, opportunities, and threats. Understanding and managing these factors helps you to develop the abilities you need to achieve your goals and progress in your career.\"}, new { original = \"Jupiter is the fifth planet from the Sun and the largest in the Solar System. It is a gas giant with a mass one-thousandth that of the Sun, but two-and-a-half times that of all the other planets in the Solar System combined. Jupiter is one of the brightest objects visible to the naked eye in the night sky, and has been known to ancient civilizations since before recorded history. It is named after the Roman god Jupiter.[19] When viewed from Earth, Jupiter can be bright enough for its reflected light to cast visible shadows,[20] and is on average the third-brightest natural object in the night sky after the Moon and Venus.\\r\\nJupiter is primarily composed of hydrogen with a quarter of its mass being helium, though helium comprises only about a tenth of the number of molecules. It may also have a rocky core of heavier elements,[21] but like the other giant planets, Jupiter lacks a well-defined solid surface. Because of its rapid rotation, the planet's shape is that of an oblate spheroid (it has a slight but noticeable bulge around the equator).\", summary = \"Jupiter is the largest planet in the solar system. It is a gas giant, and is the fifth planet from the sun.\"} }.Select(i => i.ToDictionary()).ToList(); var prompt = new FewShotPromptTemplate(example, suffix, examples); var llm = new OpenAIModel(Constants.OpenAIKey, OpenAIModelConfiguration.Default with { Temperature = 0.1f, MaxTokens = 100 }); var llmChain = new ModelChain(prompt, llm); var executor = llmChain.GetExecutor(); var answer1 = await executor.PromptAsync(\"For all its whizz-bang caper-gone-wrong energy, and for all its subsequent emotional troughs, this week’s Succession finale might have been the most important in its entire run. Because, unless I am very much wrong, Succession – a show about people trying to forcefully mount a succession – just had its succession. And now everything has to change.\\r\\nThe episode ended with Logan Roy defying his children by selling Waystar Royco to idiosyncratic Swedish tech bro Lukas Matsson. It’s an unexpected twist, like if King Lear contained a weird new beat where Lear hands the British crown to Jack Dorsey for a laugh, but it sets up a bold new future for the show. What will happen in season four? Here are some theories.\\r\\nSeason three of Succession picked up seconds after season two ended. It was a smart move, showing the immediate swirl of confusion that followed Kendall Roy’s decision to undo his father, and something similar could happen here. This week’s episode ended with three of the Roy siblings heartbroken and angry at their father’s grand betrayal. Perhaps season four could pick up at that precise moment, and show their efforts to reorganise their rebellion against him. This is something that Succession undoubtedly does very well – for the most part, its greatest moments have been those heart-thumping scenes where Kendall scraps for support to unseat his dad – and Jesse Armstrong has more than enough dramatic clout to centre the entire season around the battle to stop the Matsson deal dead in its tracks.\"); Console.WriteLine(answer1); var answer2 = await executor.PromptAsync(\"Roizman is the founder of Museum of Nevyansk Icon at Sverdlovsk region. This is the first private museum to collect icon-paintings. It is located in the city of Yekaterinburg. This museum has over 600 exhibits, including icons, gospel covers, crosses, books and wooden sculptures. The earliest icon is The Egyptian Mother of God (1734), the latest is Christ Pantocrator (1919). Roizman worked in finding, searching and restoration of the icons.\"); Console.WriteLine(answer2); > The season three finale of Succession saw Logan Roy defy his children by selling Waystar Royco to a Swedish tech bro, setting up a bold new future for the show. Season four could pick up at the moment of Logan's betrayal and focus on the Roy siblings' efforts to reorganize their rebellion against him. > Boris Roizman is the founder of the Museum of Nevyansk Icon, the first private museum to collect icon-paintings, located in Yekaterinburg, Russia. The museum houses over 600 exhibits, including icons, gospel covers, crosses, books, and wooden sculptures, ranging from The Egyptian Mother of God (1734) to Christ Pantocrator (1919). Roizman was responsible for finding, searching, and restoring the icons. Note: Roizman actual name is Yevgeny Zero-shot text classification var example = new PromptTemplate(\"Message: {message}\\n\" + \"Topic: {topic}\"); var suffix = new PromptTemplate(\"Message: {message}\\n\" + \"Topic: \"); var examples = new List<object>() { new { message = \"When the spaceship landed on Mars, the whole humanity was excited\", topic = \"space\"}, new { message = \"I love playing tennis and golf. I'm practicing twice a week.\", topic = \"sport\"}, new { message = \"Managing a team of sales people is a tough but rewarding job.\", topic = \"business\"}, }.Select(i => i.ToDictionary()).ToList(); var prompt = new FewShotPromptTemplate(example, suffix, examples); var llm = new OpenAIModel(Constants.OpenAIKey, OpenAIModelConfiguration.Default with { MaxTokens = 100 }); var llmChain = new ModelChain(prompt, llm); var executor = llmChain.GetExecutor(); var answer1 = await executor.PromptAsync(\"I am trying to cook chicken with tomatoes.\"); Console.WriteLine(answer1); var answer2 = await executor.PromptAsync(\"2022 has been the year of solo female travel, with many women taking to the road after feeling the isolation of the pandemic.\"); Console.WriteLine(answer2); > cooking > travel Keyword and Keyphrase Extraction var example = new PromptTemplate(\"Information: {information}\\n\" + \"Keywords: {keywords}\"); var suffix = new PromptTemplate(\"Information: {information}\\n\" + \"Keywords: \"); var examples = new List<object>() { new { information = \"Information Retrieval (IR) is the process of obtaining resources relevant to the information need. For instance, a search query on a web search engine can be an information need. The search engine can return web pages that represent relevant resources.\", keywords = \"information, search, resources\"}, new { information = \"David Robinson has been in Arizona for the last three months searching for his 24-year-old son, Daniel Robinson, who went missing after leaving a work site in the desert in his Jeep Renegade on June 23. \", keywords = \"searching, missing, desert\"}, new { information = \"I believe that using a document about a topic that the readers know quite a bit about helps you understand if the resulting keyphrases are of quality.\", keywords = \"document, understand, keyphrases\"}, }.Select(i => i.ToDictionary()).ToList(); var prompt = new FewShotPromptTemplate(example, suffix, examples); var llm = new OpenAIModel(Constants.OpenAIKey, OpenAIModelConfiguration.Default with { MaxTokens = 100 }); var llmChain = new ModelChain(prompt, llm); var executor = llmChain.GetExecutor(); var answer1 = await executor.PromptAsync(\"Since transformer models have a token limit, you might run into some errors when inputting large documents. In that case, you could consider splitting up your document into paragraphs and mean pooling (taking the average of) the resulting vectors.\"); Console.WriteLine(answer1); var answer2 = await executor.PromptAsync(\"If your classes are not known in advance (e.g., they are set by a user or generated on the fly), you can try zero-shot classification by either giving an instruction containing the classes or even by using embeddings to see which class label (or other classified texts) are most similar to the text\"); Console.WriteLine(answer2); > transformer, token, errors, inputting, document, paragraphs, mean pooling > classes, user, embeddings Keyword extraction is the process of identifying the main ideas from a text. Keyphrase extraction is similar, but it involves extracting multiple words. var example = new PromptTemplate(\"Information: {information}\\n\" + \"Keywords: {keywords}\"); var suffix = new PromptTemplate(\"Information: {information}\\n\" + \"Keywords: \"); var examples = new List<object>() { new { information = \"Information Retrieval (IR) is the process of obtaining resources relevant to the information need. For instance, a search query on a web search engine can be an information need. The search engine can return web pages that represent relevant resources.\", keywords = \"information retrieval, search query, relevant resource\"}, new { information = \"David Robinson has been in Arizona for the last three months searching for his 24-year-old son, Daniel Robinson, who went missing after leaving a work site in the desert in his Jeep Renegade on June 23. \", keywords = \"searching son, missing after work, desert\"}, new { information = \"I believe that using a document about a topic that the readers know quite a bit about helps you understand if the resulting keyphrases are of quality.\", keywords = \"document, help understand, resulting keyphrases\"}, }.Select(i => i.ToDictionary()).ToList(); var prompt = new FewShotPromptTemplate(example, suffix, examples); var llm = new OpenAIModel(Constants.OpenAIKey, OpenAIModelConfiguration.Default with { MaxTokens = 100 }); var llmChain = new ModelChain(prompt, llm); var executor = llmChain.GetExecutor(); var answer1 = await executor.PromptAsync(\"Since transformer models have a token limit, you might run into some errors when inputting large documents. In that case, you could consider splitting up your document into paragraphs and mean pooling (taking the average of) the resulting vectors.\"); Console.WriteLine(answer1); var answer2 = await executor.PromptAsync(\"If your classes are not known in advance (e.g., they are set by a user or generated on the fly), you can try zero-shot classification by either giving an instruction containing the classes or even by using embeddings to see which class label (or other classified texts) are most similar to the text\"); Console.WriteLine(answer2); > transformer models, token limit, large documents, splitting, mean pooling classes, zero-shot classification, embeddings, class label This time, instead of extracting one single word, we want to extract several words (known as keyphrases) from the same example. Product Description and Ad Generation var example = new PromptTemplate(\"Keywords: {keywords}\\n\" + \"Sentence: {sentence}\"); var suffix = new PromptTemplate(\"Keywords: {keywords}\\n\" + \"Sentence: \"); var examples = new List<object>() { new { keywords = \"shoes, women, $59\", sentence = \"Beautiful shoes for women at the price of $59.\"}, new { keywords = \"trousers, men, $69\", sentence = \"Modern trousers for men, for $69 only.\"}, new { keywords = \"gloves, winter, $19\", sentence = \"Amazingly hot gloves for cold winters, at $19.\"}, }.Select(i => i.ToDictionary()).ToList(); var prompt = new FewShotPromptTemplate(example, suffix, examples); var llm = new OpenAIModel(Constants.OpenAIKey, OpenAIModelConfiguration.Default with { MaxTokens = 100 }); var llmChain = new ModelChain(prompt, llm); var executor = llmChain.GetExecutor(); var answer1 = await executor.PromptAsync(\"t-shirt, men, $39\"); Console.WriteLine(answer1); var answer2 = await executor.PromptAsync(\"car, dog, 199$\"); Console.WriteLine(answer2); > Stylish t-shirts for men, only $39. > Get a car for your dog for just 199$. A little extra These examples could as well be created by use few-shots prompts to create them for us. Here is an snippet how to generate prompts based on example. var example = new PromptTemplate(\"{phrase}\\n\" + \"Prompt:\\n{prompt}\"); var suffix = new PromptTemplate(\"{phrase}\\n\" + \"Prompt:\\n\"); var examples = new List<object>() { new { phrase = \"Hugging Face a révolutionné le NLP.\\r\\nTranslation: Hugging Face revolutionized NLP.\", prompt = \"var example = new PromptTemplate(\\\"{phrase}\\\\nTranslation: {translation}\\\");\\r\\nvar suffix = new PromptTemplate(\\\"{phrase}\\\\nTranslation: \\\");\" }, new { phrase = \"Description: Fetch the companies that have less than five people in it.\\r\\nCode: SELECT COMPANY, COUNT(EMPLOYEE_ID) FROM Employee GROUP BY COMPANY HAVING COUNT(EMPLOYEE_ID) < 5;\", prompt = \"var example = new PromptTemplate(\\\"Description: {Question}\\\\nCode: {Answer}\\\");\\r\\nvar suffix = new PromptTemplate(\\\"Description: {Question}\\\\nCode: \\\");\" }, new { phrase = \"Context: NLP Cloud was founded in 2021 when the team realized there was no easy way to reliably leverage Natural Language Processing in production.\\r\\nQuestion: When was NLP Cloud founded?\\r\\nAnswer: 2021\", prompt = \"var example = new PromptTemplate(\\\"Context: {context}\\\\nQuestion: {question}\\\\nAnswer: {answer}\\\");\\r\\nvar suffix = new PromptTemplate(\\\"Context: {context}\\\\nQuestion: {question}\\\\nAnswer: \\\");\" }, }.Select(i => i.ToDictionary()).ToList(); var prompt = new FewShotPromptTemplate(example, suffix, examples); var llm = new OpenAIModel(Constants.OpenAIKey, OpenAIModelConfiguration.Default with { MaxTokens = 200 }); var llmChain = new ModelChain(prompt, llm); var executor = llmChain.GetExecutor(); var answer1 = await executor.PromptAsync(\"keyword: markets\\r\\ntweet: Take feedback from nature and markets, not from people\"); Console.WriteLine(answer1); var example = new PromptTemplate(\"Keyword: {keyword}\\nTweet: {tweet}\"); var suffix = new PromptTemplate(\"Keyword: {keyword}\\nTweet: \"); This obviously could be expanded to entire codebase generation."
  },
  "articles/prompts/getting_started.html": {
    "href": "articles/prompts/getting_started.html",
    "title": "Getting Started | DotnetPrompt",
    "keywords": "Getting Started One of the key component of any LLM and DotnetPrompt is a PromptTemplate. This article aims to clarify the concept of prompt templates and how they can assist you in producing high-quality inputs for your models. You will understand how to create prompt templates that are customized to your specific requirements and how to use few-shot learning to generate additional examples. What is a prompt template? A prompt template is a pre-defined structure that can be filled in with specific information to generate a prompt. The beauty of prompt templates lies in their ability to be highly reproducible. By providing a standardized format for prompts, prompt templates can help ensure consistency in the quality of inputs that are fed into the model. At its core, a prompt template consists of a text string or \"the template,\" which can take in parameters from the end user to generate a prompt. These parameters can include language model instructions, few-shot examples to improve the model's response, or specific questions for the model to answer. To help you better understand prompt templates, we've included a code snippet below that showcases a basic prompt template. With the ability to customize this template with specific parameters, you can generate a wide range of different prompts to suit your needs and tasks. I want you to act as a naming consultant for new companies. Here are some examples of good company names: - search engine, Google - social media, Facebook - video sharing, YouTube The name should be short, catchy and easy to remember. What is a good name for a company that makes {product}? Input Variables Input variables are the variables that are used to fill in the template string. In the example above, the input variable is a {product}. Given an input variables, the PromptTemplate can generate a prompt by filling in the template string with input values. For example, if the input value is mobile phone, the template string can be formatted by IPromptTemplate.Format method to generate the following prompt: I want you to act as a naming consultant for new companies. Here are some examples of good company names: - search engine, Google - social media, Facebook - video sharing, YouTube The name should be short, catchy and easy to remember.\" + What is a good name for a company that makes mobile phone? Create a Prompt Template You can create prompts using the PromptTemplate class. Prompt templates can take any number of input variables, and can be formatted with input values to generate a prompt. var template = \"I want you to act as a naming consultant for new companies.\\n\\n\" + \"Here are some examples of good company names:\\n\\n\" + \"- search engine, Google\\n\" + \"- social media, Facebook\\n\" + \"- video sharing, YouTube\\n\\n\" + \"The name should be short, catchy and easy to remember.\\n\\n\" + \"What is a good name for a company that makes {product}?\\n\"; var prompt = new PromptTemplate( template: template, inputVariables: new[] { \"product\" }); When you want to fill template with values you need to use Dictionary<string, string> where keys should be the same as your input variables and values could be any valid string that need to be fill in template. var values = new Dictionary<string, string>() { { \"product\", \"toy car\" } }; var finalPrompt = prompt.Format(values); You could read more about prompt template and how to use it here. Note Currently, the template should be formatted as a C# formatted string. In the future, we will add more templating languages such as Mustache. Pass few shot examples to a prompt template The FewShotPromptTemplate class combines prefixes, examples, and suffixes into a prompt that is suitable for LLMs. This allows the user to create prompts with a few examples that can significantly enhance the model's accuracy. For a complete list of examples and use cases for few-shot learning, you can refer to this page. Here's an example to demonstrate how FewShotPromptTemplate works: var prefix = new PromptTemplate(\"I want you to act as a naming consultant for new companies.\\n\" + \"Here are some examples of good company names:\"); var example = new PromptTemplate(\"- {product}, {company}\"); var suffix = new PromptTemplate(\"The name should be short, catchy and easy to remember.\\n\" + \"What is a good name for a company that makes {product}?\\n\"); var examples = new List<IDictionary<string, string>>() { new Dictionary<string, string>() { { \"product\", \"search engine\" }, { \"company\", \"Google\" } }, new Dictionary<string, string>() { { \"product\", \"social media\" }, { \"company\", \"Facebook\" } }, new Dictionary<string, string>() { { \"product\", \"video sharing\" }, { \"company\", \"YouTube\" } }, }; var prompt = new FewShotPromptTemplate(prefix, example, suffix, examples) { ExampleSeparator = \"\\n\" }; var values = new Dictionary<string, string>() { { \"product\", \"toy cars\" } }; Console.WriteLine(prompt.Format(values)); I want you to act as a naming consultant for new companies. Here are some examples of good company names: - search engine, Google - social media, Facebook - video sharing, YouTube The name should be short, catchy and easy to remember. What is a good name for a company that makes toy cars? Select examples for a prompt template If you have a large number of examples, you can use the implementation of IExampleSelector to select a subset of examples that will be most informative for the Language Model. This will help you generate a prompt that is more likely to generate a good response. There are a lot of different approaches to select examples, but currently only one is supported: LengthBasedExampleSelector which try to fit your examples into maximum allowed size of input for a model input size."
  },
  "articles/prompts/prompt_template.html": {
    "href": "articles/prompts/prompt_template.html",
    "title": "Create a prompt template | DotnetPrompt",
    "keywords": "Create a prompt template You can create prompts using the PromptTemplate class. Prompt templates can take any number of input variables, and can be formatted with input values to generate a prompt. An example prompt with no input variables var noInputPrompt = new PromptTemplate(\"Tell me a joke.\"); Console.WriteLine(noInputPrompt.Format(new Dictionary<string, string>())); > Tell me a joke. An example prompt with one input variable var oneInputPrompt = new PromptTemplate(template: \"Tell me a {adjective} joke.\", inputVariables: new[] { \"adjective\" }); var valuesOneInput = new Dictionary<string, string> { { \"adjective\", \"funny\" } }; Console.WriteLine(oneInputPrompt.Format(valuesOneInput)); > Tell me a funny joke. An example prompt with multiple input variables var multipleInputPrompt = new PromptTemplate(\"Tell me a {adjective} joke about {content}.\", new[] { \"adjective\", \"content\" }); var valuesMultipleInput = new Dictionary<string, string> { { \"adjective\", \"funny\" }, { \"content\", \"chickens\" } }; Console.WriteLine(multipleInputPrompt.Format(valuesMultipleInput)); > Tell me a funny joke about chickens. You could pass input variables into PromptTempalte constructor as a List<string> but also you could omit it, then it would be built automatically. var oneInputPrompt = new PromptTemplate(template: \"Tell me a {adjective} joke.\"); If you want to pass symbols '{' or '}' as a part of a prompt you should escape them like this new PromptTemplate(template: \"{{ \\\"code\\\": \\\"{value}\\\" }}\"); which will produce final string after formatting and set up value { \"code\": \"input value\" } When you want to fill template with values you need to use Dictionary<string, string> where keys should be the same as your input variables and values could be any valid string that need to be fill in template."
  },
  "articles/research_papers.html": {
    "href": "articles/research_papers.html",
    "title": "Research Papers | DotnetPrompt",
    "keywords": "Research Papers OpenAI Cookbook ChatML Chain of Thought Prompting A prompting technique used to encourage the model to generate a series of intermediate reasoning steps. A less formal way to induce this behavior is to include �Let�s think step-by-step� in the prompt. Resources: Chain-of-Thought Paper Step-by-Step Paper Prompt Chaining Combining multiple LLM calls together, with the output of one-step being the input to the next. Resources: PromptChainer Paper Language Model Cascades ICE Primer Book Socratic Models Chain of Thought Prompting A prompting technique used to encourage the model to generate a series of intermediate reasoning steps. A less formal way to induce this behavior is to include �Let�s think step-by-step� in the prompt. Resources: Chain-of-Thought Paper Step-by-Step Paper MemPrompt MemPrompt maintains a memory of errors and user feedback, and uses them to prevent repetition of mistakes. Resources: Paper"
  },
  "articles/tools/getting_started.html": {
    "href": "articles/tools/getting_started.html",
    "title": "Tools | DotnetPrompt",
    "keywords": "Tools Dotnet prompt provide dotnet implementation of tools useful for Natural Language Processing tasks. Most of these tools are availible for python developers natively and we trying to replicate them in C# for use with LLMs. But, all the tools provide interfaces and could be easily replaced with python binding or other implementations. Embedder Tokenizer"
  },
  "articles/usecases/chatbots.html": {
    "href": "articles/usecases/chatbots.html",
    "title": "Chatbots | DotnetPrompt",
    "keywords": "Chatbots Large Language Models are good for chatbots because they can understand natural language and generate human-like responses. With the ability to analyze and learn from vast amounts of data, these models can quickly adapt to different types of conversations and provide relevant and personalized responses. Few-shots learning example By using a ModelChain with history of previous messages you could construct chatbot capable to talk and remember previous discussion. var suffix = new PromptTemplate(\"{human_phrase}\"); var examples = new List<(string, string)>() { new(\"Hello nice to meet you.\", \"Nice to meet you too.\"), new(\"How is it going today?\", \"Not so bad, thank you! How about you?\"), new(\"I am ok, but I am a bit sad...\", \"Oh? Why that?\") }; var prompt = new ChatMLPromptTemplate(suffix, \"This is a discussion between a human and a robot. The robot is very nice and empathetic.\", examples); var llm = new ChatGptModel(Constants.OpenAIKey, ChatGptModelConfiguration.Default with { Temperature = 0.9f, MaxTokens = 100 }); var llmChain = new ModelChain(prompt, llm); var executor = llmChain.GetExecutor(); var answer1 = await executor.PromptAsync(\"I broke up with my girlfriend...\"); Console.WriteLine(answer1); var answer2 = await executor.PromptAsync(\"I won a lot of money today\"); Console.WriteLine(answer2); Conversational Chain When we combine Chain, Memory and ChatML model we could construct ultimative chatbot that could talk on any topics. ConversationalChain - Coming Soon"
  },
  "articles/usecases/docs_generation.html": {
    "href": "articles/usecases/docs_generation.html",
    "title": "Documentation Generation | DotnetPrompt",
    "keywords": "Documentation Generation Code documentation is crucial for any software development project. It helps to make code understandable, maintainable and reduces the time required to fix bugs. Documentation for classes and functions is particularly important as it gives an overview of what the code does, how to use it, and any constraints or limitations. However, writing documentation can be time-consuming and often gets neglected. In this article, we will explore how with help of DotnetPrompt a Large Language Model (LLM) can be used to automatically generate documentation for a class with minimal effort. Problem: We have a source file with a Logger class. This class provides logging functionality to other parts of the codebase. The Logger class has a predefined log messages in methods that look like this. public void GetDocumentCodes(string id, int[] codes) { WriteDebug(10001, new { id, codes }, () => $\"Reading document codes for id {id}.\"); } public void CreateMetadataCompleted(Guid id, TimeSpan timeToFilter) { WriteInfo(11004, new { id, timeToFilter }, () => $\"Get comparison metadata for id {id} in {timeToFilter:c}\"); } public void IndexClientFailed(Exception exception) { WriteError(13001, new { message = exception.Message }, () => $\"Failed to execute index client with error {exception.Message}\", exception); } We need to create documentation for this class that includes a log code, log level and log message. This documentation should be presented in a markdown table format. Solution: We will use custom Chain with an LLM to generate the documentation for the Logger class. The Chain will take the source file as input, and generate markdown table documentation for the class. Inside the chain we will setup OpenAIModel with few prompt examples. Step 1: Install Required Packages We will be using the OpenAI GPT-3 API to generate the documentation. You will need to sign up for an API key from OpenAI and install the DotnetPrompt from NuGet. > dotnet add package DotnetPrompt.All --version 1.0.0-alpha.1 Step 2: Initialize OpenAI API Key We would not use configuration or something like this, so we just store API key as a constant. public static class Constants { public const string OpenAIKey = \"YOUR-KEY\"; } Step 3: Define the Prompt Examples and setup ModelChain We will define a few prompt examples for the LLM to generate the documentation. These examples will give the LLM an idea of the format and structure we want the documentation to take. Inside out custom chain we will use basic ModelChain to make a call to LLM. private ModelChain BuildFewPromptLLModelChain() { var example = new PromptTemplate(\"Code:\\n{code}\\nTableRow: {row}\"); var suffix = new PromptTemplate(\"Code:\\n{code}\\nTableRow: \"); var examples = new List<IDictionary<string, string>>() { new Dictionary<string, string>() { { \"code\", \"public void GetDocumentCodes(string id, int[] codes)\\r\\n {\\r\\n WriteDebug(10001, new { id, codes },\\r\\n () => $\\\"Reading document metadata for id {id}.\\\");\\r\\n }\" }, { \"row\", \"| Debug | 10001 | GetDocumentCodes | Reading document metadata for id {globalId}. |\" } }, new Dictionary<string, string>() { { \"code\", \"public void CreateMetadataCompleted(Guid id, TimeSpan timeToFilter)\\r\\n {\\r\\n WriteInfo(11004, new { id, timeToFilter },\\r\\n () => $\\\"Get comparison metadata for id {id} in {timeToFilter:c}\\\");\\r\\n }\" }, { \"row\", \"| Info | 11004 | CreateComparisonMetadataCompleted | Get comparison metadata for id {id} in {timeToFilter:c} |\" } }, new Dictionary<string, string>() { { \"code\", \"public void IndexClientFailed(Exception exception)\\r\\n {\\r\\n WriteError(13001, new { message = exception.Message },\\r\\n () => $\\\"Failed to execute index client with error {exception.Message}\\\", exception);\\r\\n }\" }, { \"row\", \"| Error | 13001 | IndexClientFailed | Failed to execute index client with error {exception.Message} |\" } }, }; var prompt = new FewShotPromptTemplate(example, suffix, examples) { ExampleSeparator = \"---\" }; var model = new ModelChain(prompt, new OpenAIModel(Constants.OpenAIKey, OpenAIModelConfiguration.Default), _logger); return model; } Step 4: Build the dataflow Next we need to setup our dataflow private readonly TransformBlock<IList<ChainMessage>, ChainMessage> _finalizatorBlock; private readonly TransformManyBlock<ChainMessage, ChainMessage> _transformationBlockOne; private readonly CancellationTokenSource _cts = new(TimeSpan.FromMinutes(1)); private readonly ModelChain _llmModelChain; public ConvertDotnetTestsToMdTableChain(ILogger<ConvertDotnetTestsToMdTableChain> logger) { _logger = logger; var dataflowOptions = new ExecutionDataflowBlockOptions() { CancellationToken = _cts.Token }; // \"transform\" .cs file to list of methods _transformationBlockOne = new TransformManyBlock<ChainMessage, ChainMessage>(ReadMethodsFromFile, dataflowOptions); // LLM set up with several examples through few-prompt learning _llmModelChain = BuildFewPromptLLModelChain(); // buffer to collect rows var batchRowsBlock = new BatchBlock<ChainMessage>(100, new GroupingDataflowBlockOptions() { CancellationToken = _cts.Token }); // \"transform\" from list of rows to table _finalizatorBlock = new TransformBlock<IList<ChainMessage>, ChainMessage>(CombineRowsToTable, dataflowOptions); var linkOptions = new DataflowLinkOptions() { PropagateCompletion = true }; // set up chain _transformationBlockOne.LinkTo(_llmModelChain.InputBlock, linkOptions); _llmModelChain.OutputBlock.LinkTo(batchRowsBlock, linkOptions); batchRowsBlock.LinkTo(_finalizatorBlock, linkOptions); } The dataflow here goes like this: TransformManyBlock -> get a single ChainMessage with file name and produce ChainMessage for each method. ModelChain -> Consume ChainMessage with method and extract documentation row from it (this could be launch in parallel, so we could simultaniously generate 5-10 rows). BatchBlock -> Collect results ChainMessage from models. TransformBlock -> Combine results from BatchBlock into a final ChainMessage with a table. In fact you could pass any data between internal blocks, only first and last need to consume and return ChainMessage. The only recomendation here is to pass Id from input block to output block (without it executor would not work for example) First and last data block we would make as a field to publish them as Input and Output of our chain. public ITargetBlock<ChainMessage> InputBlock => _transformationBlockOne; public ISourceBlock<ChainMessage> OutputBlock => _finalizatorBlock; Step 5: Parse the Source File Before we can generate the markdown table documentation, we need to extract the methods of the Logger class from the source file. For that we have a _transformationBlockOne which action ReadMethodsFromFile could look like this. private IEnumerable<ChainMessage> ReadClassesFromFile(string arg) { _logger.LogInformation($\"Reading file {arg}\"); var file = File.ReadAllText(arg); var regex = new Regex(@\"\\bpublic\\svoid\\s([a-zA-Z0-9_]+)\\(([^)]*)\\)\\s*{([^{}]*(?:{[^{}]*}[^{}]*)*)}\"); var methods = regex.Matches(file); _logger.LogInformation($\"Extracted {methods.Count}\"); var fromFile = new List<ChainMessage>(); foreach (var match in methods) { fromFile.Add(new ChainMessage( new Dictionary<string, string>() { { \"code\", match.ToString() } })); } return fromFile; } Step 6: Combine rows into table private ChainMessage CombineRowsToTable(IList<ChainMessage> message) { _logger.LogInformation(\"Finalization\"); var result = message.SelectMany(i => i.Values).Where(i => i.Key == \"text\").Select(i => i.Value); var resultText = string.Join('\\n', result); return new ChainMessage(new Dictionary<string, string>() { { DefaultOutputKey, resultText } }) { Id = message.First().Id }; } Step 7: Run method We need implemetation of Run method. Here we consume input ChainMessage with a single value - file name and post it to dataflow. public bool Run(ChainMessage message) { if (InputBlock.Completion.IsCompleted) { throw new InvalidOperationException(\"This chain would not accept any more messages\"); } var launched = InputBlock.Post(message); InputBlock.Complete(); return launched; } public void Cancel() { _cts.Cancel(); _llmModelChain.Cancel(); } Note the Complete method. We telling our chain that no more data will be added after inital file name. It's important to complete this one, because otherwise BufferBlock will be waiting forever or until it capacity full. Step 8: Generate the Markdown Table Documentation The usage of the chain is starighforward: we run the chain and wait for completion: var chain = new ConvertDotnetTestsToMdTableChain(TestLogger.Create<ConvertDotnetTestsToMdTableChain>()); var input = new Dictionary<string, string>() { { \"file\", @\"Data\\Logger.cs\" } }; chain.Run(new ChainMessage(input)); var result = await chain.OutputBlock.ReceiveAsync(); var resultText = result.Values[\"table\"]; Console.WriteLine(resultText); Final result Standard Output: | Info | 17001 | WorkerStart | Start worker | | Info | 17003 | StartProcessingDocument | Start processing document | | Info | 17004 | EndProcessingDocument | End processing document. ElapsedMillisecond: {elapsedMillisecond} | | Error | 17100 | ProcessDocumentHandledException | Processing Handled Exception | | Warning | 171001 | ChangeTypeArgumentOutOfRangeWarning | Wrong change type. Exception message: {exception.Message} | And here is a log of how our chain worked 2023-03-02T23:15:09 | Information | Reading file Data\\Logger.cs 2023-03-02T23:15:09 | Information | Extracted 5 methods 2023-03-02T23:15:09 | Trace | Sending LLM request 2023-03-02T23:15:10 | Information | Result of ModelChain: | Info | 17001 | WorkerStart | Start worker | 2023-03-02T23:15:10 | Trace | Sending LLM request 2023-03-02T23:15:11 | Information | Result of ModelChain: | Info | 17003 | StartProcessingDocument | Start processing document | 2023-03-02T23:15:11 | Trace | Sending LLM request 2023-03-02T23:15:13 | Information | Result of ModelChain: | Info | 17004 | EndProcessingDocument | End processing document. ElapsedMillisecond: {elapsedMillisecond} 2023-03-02T23:15:13 | Trace | Sending LLM request 2023-03-02T23:15:14 | Information | Result of ModelChain: | Error | 17100 | ProcessDocumentHandledException | Processing Handled Exception | 2023-03-02T23:15:14 | Trace | Sending LLM request 2023-03-02T23:15:16 | Information | Result of ModelChain: | Warning | 171001 | ChangeTypeArgumentOutOfRangeWarning | Wrong change type. Exception message: {exception.Message} | 2023-03-02T23:15:16 | Information | Finalization Conclusion In this article, we have seen how an LLM can be used to automatically generate documentation for a class. We used the OpenAI API to generate markdown table documentation for a Logger class by defining a few prompt examples and parsing the source file. While the documentation generated by the LLM may not be perfect, it can serve as a starting point for further refinement and can save developersa significant amount of time. The obvious improvement would be to provide list of examples as a parameter to make this chain suitable to generate documentation based on any kind of methods. This approach can be extended to generate documentation for other classes and functions in a codebase, making documentation a less time-consuming and tedious task."
  },
  "articles/usecases/question_answering.html": {
    "href": "articles/usecases/question_answering.html",
    "title": "Question Answering | DotnetPrompt",
    "keywords": "Question Answering Question answering is an important task in natural language processing that involves providing answers to questions based on a given context. With recent advancements in large language models, such as OpenAI's GPT series, it has become easier to build highly accurate question answering systems. In this article, we will explore how to build a simple question answering system using the DotnetPrompt and OpenAI API. Few-shots learning example By using a ModelChain with history of previous messages and context you could construct question answering chain. var llm = new ChatGptModel(Constants.OpenAIKey, ChatGptModelConfiguration.Default with { Temperature = 0, MaxTokens = 100 }); var llmChain = new QuestionAnsweringChain(llm); var executor = llmChain.GetExecutor(); var answer1 = await executor.PromptAsync(new { context = \"OpenAI offers several plans for GPT-3, ranging from a free tier to paid plans with larger amounts of access and dedicated support. The recommended plan for GPT-3 would depend on the specific needs and use case of the individual or organization. However, the most commonly used paid plan for GPT-3 is the \\\"Pro\\\" plan, which provides a significant amount of access to the API and is suitable for most applications.\", question = \"Which plan is recommended for GPT-3?\" }.ToDictionary()); Console.WriteLine(answer1[\"answer\"]); var answer12 = await executor.PromptAsync(new { context = \"GPT-3 supports many different natural languages, including English, Spanish, French, German, Italian, Dutch, Portuguese, Japanese, Korean, Chinese, and more. However, English is the language that GPT-3 has been most extensively trained on, and for which it has produced the most impressive results. Therefore, English is generally considered the most preferable language for GPT-3.\", question = \"Which language is preferable for GPT-3?\" }.ToDictionary()); Console.WriteLine(answer12[\"answer\"]); Console.WriteLine(\"---\"); var llm2 = new OpenAIModel(Constants.OpenAIKey, OpenAIModelConfiguration.Default with { Temperature = 0, MaxTokens = 100 }); var llmChain2 = new QuestionAnsweringChain(llm2); var executor2 = llmChain2.GetExecutor(); var answer2 = await executor2.PromptAsync(new { context = \"OpenAI offers several plans for GPT-3, ranging from a free tier to paid plans with larger amounts of access and dedicated support. The recommended plan for GPT-3 would depend on the specific needs and use case of the individual or organization. However, the most commonly used paid plan for GPT-3 is the \\\"Pro\\\" plan, which provides a significant amount of access to the API and is suitable for most applications.\", question = \"Which plan is recommended for GPT-3?\" }.ToDictionary()); Console.WriteLine(answer2[\"answer\"]); var answer22 = await executor.PromptAsync(new { context = \"GPT-3 supports many different natural languages, including English, Spanish, French, German, Italian, Dutch, Portuguese, Japanese, Korean, Chinese, and more. However, English is the language that GPT-3 has been most extensively trained on, and for which it has produced the most impressive results. Therefore, English is generally considered the most preferable language for GPT-3.\", question = \"Which language is preferable for GPT-3?\" }.ToDictionary()); Console.WriteLine(answer22[\"answer\"]); Web Question Answering When we combine Chain, Web parse and Vector Database we could achieve a chain what could answer on questions on any website. WebQAChain - Coming Soon"
  },
  "articles/usecases/summarization.html": {
    "href": "articles/usecases/summarization.html",
    "title": "Summarizing Texts | DotnetPrompt",
    "keywords": "Summarizing Texts Summarizing texts is an essential task for anyone who deals with large amounts of information. It can help save time and improve comprehension by condensing a lengthy text into its most crucial points. Fortunately, with the development of artificial intelligence, we can now use machine learning models to summarize texts automatically. In this article, we will look at how to use the DotnetPrompt library to summarize texts. Summarizing Texts Using Few-Shots DotnetPrompt To summarize texts using DotnetPrompt, we will follow these simple steps: Define the Prompt Template A prompt template is a string that defines the example format for the text to be summarized. It should include placeholders for the original text and the summary. For example: var template = new PromptTemplate(\"Original: {original}\\nSummary: {summary}\"); Define the Suffix Template The suffix template is a string that defines the output format for the summary. It should include a placeholder for the original text. For example: var suffix = new PromptTemplate(\"Original: {original}\\nSummary: \"); Prepare Examples Prepare a list of examples that the model can learn from. Each example should include the original text and its summary. For example: var examples = new List<object>() { new { original = \"This is an example of the original text.\", summary = \"This is a summary of the original text.\" }, // Add more examples here }.Select(i => i.ToDictionary()).ToList(); Create the Few-Shot Prompt Template Create a new Few-Shot Prompt Template object by passing in the prompt and suffix templates and the list of examples. For example: var prompt = new FewShotPromptTemplate(template, suffix, examples); Load the Language Model Create a new instance of the OpenAI language model using your OpenAI API key. For example: var llm = new OpenAIModel(Constants.OpenAIKey, OpenAIModelConfiguration.Default with { Temperature = 0.1f, MaxTokens = 100 }); Create the ModelChain Create a new instance of the Model Chain class by passing in the Few-Shot Prompt Template and the OpenAI language model. For example: var llmChain = new ModelChain(prompt, llm); Generate the Summary Call the PromptAsync method on the ModelChain instance, passing in the original text. The method will return a string containing the summary. For example: var summary = await llmChain.PromptAsync(\"This is the original text to be summarized.\"); In this article, we have looked at how to summarize texts using the DotnetPrompt library. We have seen that it is a straightforward process that involves defining the prompt and suffix templates, preparing examples, creating a few-shot prompt template, loading the language model, creating the model chain, and generating the summary. With this knowledge, you should be able to summarize texts with ease using DotnetPrompt. Summarizing Documents Using Summarizing Chain Comming soon"
  },
  "index.html": {
    "href": "index.html",
    "title": "Welcome to DotnetPrompt | DotnetPrompt",
    "keywords": "Welcome to DotnetPrompt Welcome to our library, which is designed to support the development of cutting-edge applications powered by Large Language Models (LLMs) in dotnet. As you may know, LLMs are an exciting and rapidly-evolving technology that offers developers unprecedented natural language processing and generation capabilities. However, LLMs can achieve their full potential when used in conjunction with other sources of computation or knowledge. Our library helps you integrate LLMs with other tools and resources to create powerful and sophisticated applications. Some examples of solutions that you could create using our library include: Summarization Question Answering Code Generation Chatbots and much more Getting Started You could start with the below guide for a walkthrough of how to get started using DotnetPrompt to create an Language Model application. Getting Started Documentation Basic Blocks Several basic components are available in DotnetPrompt, and several more are coming soon. We provide how-to guides to get started, examples, and reference docs for each component. Prompts: Prompts are how you communicate with LLMs. This component includes prompt management, optimization, and usage of few-shot prompts. LLMs: LLM is a generic interface for Large Language Models provider. We have implementation for some widespread providers and standard utilities for working with them out of the box. Chains: Chains are sequences of LLMs or a different utility combined in Dataflow to achieve some tasks. With DotnetPrompt, you get a standard interface for chains and several commonly used implementations. Tools: Different valuable tools for building Nature Language applications, without need to get into python. Use Cases DotnetPrompt provides examples of several common use cases, and this documentation offers guidance and assistance on how to use the components in different ways. Summarization: Summarizing longer documents into shorter, more condensed chunks of information. Question Answering: This technique involves utilizing only the information in provided documents to construct an answer to a given question. Docs Generation: Generating documentation to a source file. This is a common use case for many applications, and DotnetPrompt provides some prompts/chains for assisting in this. Chatbots: Since language models are good at producing text, that makes them ideal for creating chatbots. More use cases coming soon; subscribe to @DotnetPrompt to be updated Reference Docs All of DotnetPrompt's reference documentation, in one place. Full documentation on all methods, classes Reference Documentation Additional Resources List of related research papers: there are a lot of work going to invent better ways to work with LLMs. Some ideas of this framework is based on papers we listed there. Note This project initially was started as a port of popular Python framework LangChain. We're so grateful to the LangChain team for creating such an awesome Python framework, which inspired us to create the port that allows the dotnet community to use LLMs with ease."
  }
}